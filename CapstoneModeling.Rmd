---
title: "EDA"
author: "Imogen Holdsworth" 
date: "2024-09-20"
output: 
  html_document: 
    code_folding: hide  
    number_sections: no
    toc: yes
editor_options: 
  chunk_output_type: inline
---

# Introduction: 

Home Credit aims to promote financial inclusion by offering accessible loans to customers with limited or no formal credit history. To achieve this, Home Credit must accurately assess default risk, ensuring that loans are responsibly distributed without increasing risk exposure. This project seeks to leverage client data—such as demographics, loan history, and behavior—to develop a model that predicts the likelihood of default. The solution will improve risk management, reduce losses, expand the company’s market reach, and enhance customer experience, all while ensuring higher model accuracy measured by the ROC curve.

This analysis includes the following data sources:

The application data (split into test and train- where the test data does not contain the target variable). This includes static data on all the applicants, where each row is one loan. 

The prior application data includes data on the previous applications for Home Credit loans of clients who have loans in the sample, there is one row for each previous application related to loans in the data set.

## Data and package load

```{r setup, include=FALSE}

knitr::opts_chunk$set(warning = FALSE)

pacman::p_load(e1071,ggplot2, dplyr, skimr, caret, tidyverse, knitr, janitor, randomForest, gridExtra, corrplot, DMwR, pROC)

app_train <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_train.csv")

app_test <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_test.csv")

#prior_app <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/previous_application.csv")

#bureau <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/bureau.csv")

dd <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/HomeCredit_columns_description.csv")
```

# Application Data:

The following section will explore the application data, for both the test and train groups. The goal is to discover any important relationships or patterns that exist in the data, and understand how they impact loan repayment by clients. We will work to begin preliminary identification of potential predictors of clients loan repayment ability, and set the foundation for our analysis.  


## Data Overview

```{r data overview train application data}
# Basic data overview

# what does the data frame look like?
head(app_train)

# Data Structure
str(app_train)

# Data Summary
summary(app_train)

dim(app_train)
```

The data set on current applicants split into the train group consists of 307,511 observations across 123 variables.

The problem here is binary classification, where we want to predict 1 (if we think our applicant is a defaulter), and 0 (if we think our applicant is not a defaulter). 

In thinking about our models performance we will likely want to avoid miss-classifications if possible as it will be a costly mistake. 


## Missing Data

Now we will begin to identify where we have missing instances of data in the data set.


```{r find missing data in the application train dataset}

# locate only columns with missing values

missing_values <- colSums(is.na(app_train))

missing_values <- missing_values[missing_values > 0]


# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```

Here we can see that there is a ton of missing data, 38 of our 123 variables have over 50% of the data missing. The top few variables with missing data include, the Age of the client's car,EXT_SOURCE_1(Normalized score from an external data source), information about the building where the client lives (APARTMENTS_AVG, APARTMENTS_MODE, APARTMENTS_MEDI), information about the basement size of the clients building(BASEMENTAREA_AVG, BASEMENTAREA_MODE, BASEMENTAREA_MEDI), information on the buildings use (YEARS_BEGINEXPLUATATION_AVG, YEARS_BEGINEXPLUATATION_MODE, YEARS_BEGINEXPLUATATION_MEDI), data on the construction dates of the building (YEARS_BUILD_AVG, YEARS_BUILD_MODE, YEARS_BUILD_MEDI), and more. We will need to determine with more than 50% of the data missing, whether or not we drop the variables completely  or impute using the data that we have. If the variables are not very valuable to our model it is likely we will see better success with dropping them. Further in our analysis we will confront these data issues as we learn more about the relationships that exist in the data set.  

Notes:
    
  When we drop the data numeric data should be imputed with the median, or with KNN/regression if there's a lot of missing data.
  For categorical variables we should use the mode for imputation. 
  
  
```{r}
missing_values <- colSums(is.na(app_test))

missing_values <- missing_values[missing_values > 0]


# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_test)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```

  
Drop the variables containing a large amount of missing data, drop variables with more than 45% of data missing, except Ext Source 1

```{r}

app_train <- app_train %>% select(-c("COMMONAREA_AVG", "COMMONAREA_MODE", "COMMONAREA_MEDI", "NONLIVINGAPARTMENTS_AVG", "NONLIVINGAPARTMENTS_MODE","NONLIVINGAPARTMENTS_MEDI","LIVINGAPARTMENTS_AVG","LIVINGAPARTMENTS_MODE","LIVINGAPARTMENTS_MEDI", "FLOORSMIN_AVG","FLOORSMIN_MODE","FLOORSMIN_MEDI", "YEARS_BUILD_AVG","YEARS_BUILD_MODE","YEARS_BUILD_MEDI","OWN_CAR_AGE","LANDAREA_AVG","LANDAREA_MODE","LANDAREA_MEDI","BASEMENTAREA_AVG","BASEMENTAREA_MODE","BASEMENTAREA_MEDI","NONLIVINGAREA_AVG", "NONLIVINGAREA_MODE", "NONLIVINGAREA_MEDI","ELEVATORS_AVG","ELEVATORS_MODE","ELEVATORS_MEDI", "APARTMENTS_AVG", "APARTMENTS_MODE","APARTMENTS_MEDI", "ENTRANCES_AVG","ENTRANCES_MODE","ENTRANCES_MEDI", "LIVINGAREA_AVG","LIVINGAREA_MODE","LIVINGAREA_MEDI","FLOORSMAX_AVG","FLOORSMAX_MODE","FLOORSMAX_MEDI","YEARS_BEGINEXPLUATATION_AVG","YEARS_BEGINEXPLUATATION_MODE", "YEARS_BEGINEXPLUATATION_MEDI","TOTALAREA_MODE"))

app_test <- app_test %>% select(-c("COMMONAREA_AVG", "COMMONAREA_MODE", "COMMONAREA_MEDI", "NONLIVINGAPARTMENTS_AVG", "NONLIVINGAPARTMENTS_MODE","NONLIVINGAPARTMENTS_MEDI","LIVINGAPARTMENTS_AVG","LIVINGAPARTMENTS_MODE","LIVINGAPARTMENTS_MEDI", "FLOORSMIN_AVG","FLOORSMIN_MODE","FLOORSMIN_MEDI", "YEARS_BUILD_AVG","YEARS_BUILD_MODE","YEARS_BUILD_MEDI","OWN_CAR_AGE","LANDAREA_AVG","LANDAREA_MODE","LANDAREA_MEDI","BASEMENTAREA_AVG","BASEMENTAREA_MODE","BASEMENTAREA_MEDI","NONLIVINGAREA_AVG", "NONLIVINGAREA_MODE", "NONLIVINGAREA_MEDI","ELEVATORS_AVG","ELEVATORS_MODE","ELEVATORS_MEDI", "APARTMENTS_AVG", "APARTMENTS_MODE","APARTMENTS_MEDI", "ENTRANCES_AVG","ENTRANCES_MODE","ENTRANCES_MEDI", "LIVINGAREA_AVG","LIVINGAREA_MODE","LIVINGAREA_MEDI","FLOORSMAX_AVG","FLOORSMAX_MODE","FLOORSMAX_MEDI","YEARS_BEGINEXPLUATATION_AVG","YEARS_BEGINEXPLUATATION_MODE", "YEARS_BEGINEXPLUATATION_MEDI","TOTALAREA_MODE"))

```

Re inspect the amount of missing data:

```{r}
# locate only columns with missing values

missing_values <- colSums(is.na(app_train))

missing_values <- missing_values[missing_values > 0]

# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)

```


Model the distribution of the missing variables to see if we should impute with mean or median:

```{r, warning = FALSE}
# AMT_REQ_CREDIT_BUREAU_HOUR: This variable captures how many times a credit bureau has been contacted about the client’s credit history within a one-hour window

# Create individual plots for each variable
p1 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_HOUR)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_HOUR")

p2 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_DAY)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_DAY")

p3 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_WEEK)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_WEEK")

p4 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_MON)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_MON")

p5 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_QRT)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_QRT")

p6 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_YEAR)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_YEAR")

# Combine the plots in a grid
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)



```

This helps us understand a few things, there seems to be some outlier in AMT_REQ_CREDIT_BUREAU_QRT, and also all the values are categorigcal, only taking whole values between 0 and at most 200 - except AMT_REQ_CREDIT_BUREAU_DAY which does take half values and is the number of enquiries to the Credit Beura about that client in the last day. We can use the mode to impute these missing data for these 6 variables. 

- These variables are all the same measure just over differnt periods of time, its not likley we would want to keep them all in one model but we can determine which ones to drop later on. 

```{r}
# Impute the above variables for the missing values with the mode of 0

credit_bureau_vars <- c("AMT_REQ_CREDIT_BUREAU_HOUR", "AMT_REQ_CREDIT_BUREAU_DAY", 
                        "AMT_REQ_CREDIT_BUREAU_WEEK", "AMT_REQ_CREDIT_BUREAU_MON", 
                        "AMT_REQ_CREDIT_BUREAU_QRT", "AMT_REQ_CREDIT_BUREAU_YEAR")

# Loop through each variable and replace missing values with the mode (0)
for (var in credit_bureau_vars) {
  app_train[[var]][is.na(app_train[[var]])] <- 0
}

# perform the same imputation across the test set. 

for (var in credit_bureau_vars) {
  app_test[[var]][is.na(app_test[[var]])] <- 0
}

```

Now we are left with: 
OBS_30_CNT_SOCIAL_CIRCLE and OBS_60_CNT_SOCIAL_CIRCLE (Observations):
These variables represent how many people in the client's social circle have been observed with overdue payments (30 or 60 days past due).
Essentially, these variables measure the number of people in the client's social network who are being tracked or monitored regarding their credit status (30 or 60 days overdue).
DEF_30_CNT_SOCIAL_CIRCLE and DEF_60_CNT_SOCIAL_CIRCLE (Defaults):
These variables represent how many people in the client's social circle have actually defaulted on payments (i.e., 30 or 60 days overdue and marked as a default).
So, these are the number of people who not only have overdue payments but are officially classified as being in default for 30 or 60 days.

These could be valauble indicators in our model, lets take a look at the data distribution and see how we can impute the missing values:

```{r}
# Create individual plots for each variable

p7 <- ggplot(app_train, aes(x = OBS_30_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "OBS_30_CNT_SOCIAL_CIRCLE")

p8 <- ggplot(app_train, aes(x = OBS_60_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "OBS_60_CNT_SOCIAL_CIRCLE")

p9 <- ggplot(app_train, aes(x = DEF_30_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DEF_30_CNT_SOCIAL_CIRCLE")

p10 <- ggplot(app_train, aes(x = DEF_60_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DEF_60_CNT_SOCIAL_CIRCLE")


# Combine the plots in a grid
grid.arrange(p7, p8, p9, p10, ncol = 2)
```
We can see that this data is heavily right skewed, we will also use the mode here to impute the missing values in order to not alter the distribution of the data. We can also keep in mind for future analysis this data is heavily skewed and contains some outliers. 


```{r}
# impute the missing data for the variables above:


social_bureau_vars <- c("OBS_30_CNT_SOCIAL_CIRCLE", "OBS_60_CNT_SOCIAL_CIRCLE", 
                        "DEF_30_CNT_SOCIAL_CIRCLE", "DEF_60_CNT_SOCIAL_CIRCLE")

# Loop through each variable and replace missing values with the mode (0)
for (var in social_bureau_vars) {
  app_train[[var]][is.na(app_train[[var]])] <- 0
}

# perform the same imputation across the test set. 

for (var in social_bureau_vars) {
  app_test[[var]][is.na(app_test[[var]])] <- 0
}


```

Now we are left with: 

1. AMT_GOODS_PRICE (278 missing values):
Description: This variable represents the price of the goods that the client is asking for a loan to purchase (for consumer loans). It reflects the amount the loan is intended to cover.
Recommendation:
If the missing values are few (278 out of a large dataset), you could consider imputing the missing values with the median of the variable, as the price might vary significantly based on the goods purchased.
If you have domain knowledge, you could also use the mean or a default value depending on the typical goods price in your dataset.
2. AMT_ANNUITY (12 missing values):
Description: This represents the loan annuity, i.e., the amount the client will pay in periodic installments to repay the loan (principal + interest).
Recommendation: Since there are only 12 missing values, using the median or mean annuity from the training data would be appropriate. The median is generally safer if the data is skewed, as it won't be influenced by outliers.
3. CNT_FAM_MEMBERS (2 missing values):
Description: This variable represents the number of family members the client has.
Recommendation: With only 2 missing values, you could:
Impute with the mode (most common family size) if it is known, as family size tends to be a categorical variable with common values (e.g., 2, 3, 4).
Alternatively, you could impute with the median, especially if you find that most clients have a similar number of family members.
4. DAYS_LAST_PHONE_CHANGE (1 missing value):
Description: This variable captures how many days before the loan application the client changed their phone number.
Recommendation: Since there's only 1 missing value, you could simply impute it with the median value of the existing data. Given that this is a continuous variable representing time, the median is a safe option to prevent outliers from affecting the imputation.
```{r}

p11 <- ggplot(app_train, aes(x = AMT_GOODS_PRICE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_GOODS_PRICE")

p12 <- ggplot(app_train, aes(x = AMT_ANNUITY)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_ANNUITY")
p13 <- ggplot(app_train, aes(x = CNT_FAM_MEMBERS)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "CNT_FAM_MEMBERS")

p14 <- ggplot(app_train, aes(x = DAYS_LAST_PHONE_CHANGE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DAYS_LAST_PHONE_CHANGE")


# Combine the plots in a grid
grid.arrange(p11,p12,p13,p14, ncol = 2)
summary(app_train$DAYS_LAST_PHONE_CHANGE)
```

lets use the mode for the cnt fam members, median for the amt goods price and amt annuity and days last phone cahnge

```{r}

# Impute AMT_GOODS_PRICE with median

app_train$AMT_GOODS_PRICE[is.na(app_train$AMT_GOODS_PRICE)] <- median(app_train$AMT_GOODS_PRICE, na.rm = TRUE)

# Impute AMT_ANNUITY with median

app_train$AMT_ANNUITY[is.na(app_train$AMT_ANNUITY)] <- median(app_train$AMT_ANNUITY, na.rm = TRUE)

# Impute CNT_FAM_MEMBERS with mode (most common value)

mode_fam_members <- as.numeric(names(sort(table(app_train$CNT_FAM_MEMBERS), decreasing = TRUE)[1]))
app_train$CNT_FAM_MEMBERS[is.na(app_train$CNT_FAM_MEMBERS)] <- mode_fam_members

# Impute DAYS_LAST_PHONE_CHANGE with median

app_train$DAYS_LAST_PHONE_CHANGE[is.na(app_train$DAYS_LAST_PHONE_CHANGE)] <- median(app_train$DAYS_LAST_PHONE_CHANGE, na.rm = TRUE)


# Do the same for the test data:

app_test$AMT_GOODS_PRICE[is.na(app_test$AMT_GOODS_PRICE)] <- median(app_test$AMT_GOODS_PRICE, na.rm = TRUE)

app_test$AMT_ANNUITY[is.na(app_test$AMT_ANNUITY)] <- median(app_test$AMT_ANNUITY, na.rm = TRUE)

mode_fam_members_test <- as.numeric(names(sort(table(app_test$CNT_FAM_MEMBERS), decreasing = TRUE)[1]))
app_test$CNT_FAM_MEMBERS[is.na(app_test$CNT_FAM_MEMBERS)] <- mode_fam_members_test

app_test$DAYS_LAST_PHONE_CHANGE[is.na(app_test$DAYS_LAST_PHONE_CHANGE)] <- median(app_test$DAYS_LAST_PHONE_CHANGE, na.rm = TRUE)
```


Lets re run the missing data list:

```{r}
missing_values <- colSums(is.na(app_train))

missing_values <- missing_values[missing_values > 0]

# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```
Great! the only missing values we have now are just the external sources, we will deal with those soon

Before we begin some visualizations lets reformat age in the train and test data to make sure that it is in years instead of days for better understanding.

```{r fix age}

app_train$Age_in_Years <- -app_train$DAYS_BIRTH / 365
app_test$Age_in_Years <- -app_test$DAYS_BIRTH / 365

app_train <- app_train %>% select(-DAYS_BIRTH)
app_test <- app_test %>% select(-DAYS_BIRTH)
summary(app_train$Age_in_Years)
head(app_train)
```

```{r}
categorical_vars <- app_train %>% select(where(is.character))

# Check for missing values in categorical variables
missing_values_categorical <- sapply(categorical_vars, function(x) sum(is.na(x)))

# Create a data frame to display the results
 data.frame(
  Variable = names(missing_values_categorical),
  MissingCount = missing_values_categorical
)

 unique_values_list <- sapply(categorical_vars, unique)

# Convert the list to a more readable format, collapsing the unique values
data.frame(
  Variable = names(unique_values_list),
  Unique_Values = sapply(unique_values_list, function(x) paste(x, collapse = ", ")),
  stringsAsFactors = FALSE
)

 
 
app_train<- app_train %>% filter(CODE_GENDER != "XNA")
app_train<- app_train %>% filter(ORGANIZATION_TYPE != "XNA")

# Check to make sure XNA is removed
unique(app_train$CODE_GENDER)
unique(app_train$ORGANIZATION_TYPE)

```



Distribution of the target variable:
```{r}
app_train |>
  group_by(TARGET) |>
  summarise(Count = n()) |>
  mutate(Proportion = Count / sum(Count))

ggplot(app_train, aes(x = as.factor(TARGET))) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Target Variable (Loan Default)", x = "Loan Default (1 = Yes, 0 = No)", y = "Count")

```
The data for the Target in the training set is highly skewed, in our case we want to aviod giving a loan to someone who will default, but at the same time not be overly restrictive as we want to contineu our companies mission of loan inclusivity. We may want to consider upsampling the data as it will allow us to keep all the data, and it would help increase the sensitivity to defaults. But we can investigate both and see what kind of affects it has on the modeling. 


Our data is very imbalanced in regards to the target variable, this will impact our analysis and performance grading of the model. Here the majority class is no loan defaults with a proportion of 91.92%, our models will have to perform better than that in order to beat a majority class model. 

clean data with facorizing needed variables

```{r, include = FALSE}
summary(app_train)

character_vars_subset <- app_train %>% select_if(is.character)

# View the structure of only the character variables
str(character_vars_subset)


# Create a list of character variables

character_vars <- c("NAME_CONTRACT_TYPE",
                    "CODE_GENDER",
                    "FLAG_OWN_CAR",
                    "FLAG_OWN_REALTY" ,
                    "NAME_TYPE_SUITE",
                    "NAME_INCOME_TYPE",
                    "NAME_EDUCATION_TYPE", 
                     "NAME_FAMILY_STATUS",
                    "NAME_HOUSING_TYPE",
                    "OCCUPATION_TYPE",
                    "WEEKDAY_APPR_PROCESS_START",
                    "ORGANIZATION_TYPE",
                    "FONDKAPREMONT_MODE",
                    "HOUSETYPE_MODE",
                    "WALLSMATERIAL_MODE",
                    "EMERGENCYSTATE_MODE")
                    

app_train[character_vars] <- lapply(app_train[character_vars], as.factor)

```


  
```{r}
# Investigae integer values that might need to be factored
integer_vars <- names(app_train)[sapply(app_train, is.integer)]

# Print integer variable names
print(integer_vars)

# unique(app_train$) 


integer_vars_subset <- app_train %>% select_if(is.integer)

# View the structure of only the integer variables
str(integer_vars_subset)

  
 
 integer_vars_to_factor <- c("FLAG_MOBIL",
                             "FLAG_EMP_PHONE",
                             "FLAG_WORK_PHONE",
                             "FLAG_CONT_MOBILE",
                             "FLAG_PHONE",
                             "FLAG_EMAIL",
                             "REG_REGION_NOT_LIVE_REGION",
                             "REG_REGION_NOT_WORK_REGION",
                            "LIVE_REGION_NOT_WORK_REGION",
                           "REG_CITY_NOT_LIVE_CITY",
                          "LIVE_CITY_NOT_WORK_CITY",
                          "FLAG_DOCUMENT_2",
                          "FLAG_DOCUMENT_3",
                          "FLAG_DOCUMENT_4",
                          "FLAG_DOCUMENT_5",
                          "FLAG_DOCUMENT_6",
                          "FLAG_DOCUMENT_7",
                          "FLAG_DOCUMENT_8",
                          "FLAG_DOCUMENT_9",
                          "FLAG_DOCUMENT_10",
                          "FLAG_DOCUMENT_11",
                          "FLAG_DOCUMENT_12",
                          "FLAG_DOCUMENT_13",
                          "FLAG_DOCUMENT_14",
                          "FLAG_DOCUMENT_15",
                          "FLAG_DOCUMENT_16",
                          "FLAG_DOCUMENT_17",
                          "FLAG_DOCUMENT_18",
                          "FLAG_DOCUMENT_19",
                          "FLAG_DOCUMENT_20",
                          "FLAG_DOCUMENT_21",
                          "REG_CITY_NOT_WORK_CITY")
 
app_train[integer_vars_to_factor] <- lapply(app_train[integer_vars_to_factor], as.factor)

app_train$REGION_RATING_CLIENT <- factor(app_train$REGION_RATING_CLIENT, ordered = TRUE)
app_train$REGION_RATING_CLIENT_W_CITY <- factor(app_train$REGION_RATING_CLIENT_W_CITY, ordered = TRUE)


str(app_train)


```
  
AMT_REQ_CREDIT_BUREAU_HOUR: Number of credit bureau inquiries in the last hour.
AMT_REQ_CREDIT_BUREAU_DAY: Number of credit bureau inquiries in the last day.
AMT_REQ_CREDIT_BUREAU_WEEK: Number of credit bureau inquiries in the last week.
AMT_REQ_CREDIT_BUREAU_MON: Number of credit bureau inquiries in the last month.
AMT_REQ_CREDIT_BUREAU_QRT: Number of credit bureau inquiries in the last quarter.
AMT_REQ_CREDIT_BUREAU_YEAR: Number of credit bureau inquiries in the last year.
These variables represent count data—discrete numeric values reflecting the number of credit inquiries over different timeframes.





Investigate the relationships that exist in our current dataset:

# plot different combinations of variables to explore linear separability


  
  # Box plot for income vs loan default
ggplot(app_train, aes(x = as.factor(TARGET), y = AMT_INCOME_TOTAL)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Income vs Loan Default", x = "Loan Default (0 = No, 1 = Yes)", y = "Income") +
  theme_minimal()
  
  
  ggplot(app_train, aes(x = Age_in_Years , y = AMT_CREDIT , color = TARGET)) +
  geom_point(size = 2) +
  labs(title = "AMT_INCOME_TOTAL vs AMT_CREDIT ", x = "AMT_INCOME_TOTAL", y = "AMT_CREDIT ") +
  theme_minimal()


# check correlation

```{r}

# Subset of most important variables
important_vars <- c("AMT_CREDIT", "AMT_ANNUITY", "EXT_SOURCE_2", "DAYS_EMPLOYED", "Age_in_Years", "EXT_SOURCE_3", "EXT_SOURCE_1", "TARGET")

# Correlation matrix for important variables only
correlations_subset <- cor(app_train[important_vars], use = "complete.obs")

# Plot the subset correlation matrix
corrplot(correlations_subset, method = "color", type = "upper", tl.cex = 0.8, tl.srt = 45)

```





```{r}

# investigate Income 

ggplot(app_train, aes(x = as.factor(TARGET), y = AMT_INCOME_TOTAL)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Income vs Loan Default", x = "Loan Default", y = "Income") +
  theme_minimal()

ggplot(app_train, aes(x = "", y = AMT_INCOME_TOTAL)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Box Plot for Income (AMT_INCOME_TOTAL)", y = "Income") +
  theme_minimal()

ggplot(app_train, aes(x = "", y = AMT_CREDIT)) +
  geom_boxplot(fill = "purple") +
  labs(title = "Box Plot for  (AMT_CREDIT)", y = "Amount of Credit") +
  theme_minimal()
ggplot(app_train, aes(x = "", y = AMT_ANNUITY)) +
  geom_boxplot(fill = "brown") +
  labs(title = "Box Plot for (AMT_ANNUITY)", y = "Amount Annuity") +
  theme_minimal()
ggplot(app_train, aes(x = Age_in_Years, fill = as.factor(TARGET))) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  labs(title = "Age Distribution by Loan Defaulting", x = "Age (Years)", y = "Count") +
  theme_minimal()

```



Key Variables to Investigate with Box Plots:

Income (AMT_INCOME_TOTAL)
Loan Amount (AMT_CREDIT)
Annuity ()
Family Size (CNT_FAM_MEMBERS)
Age (Age_in_Years)


investigate super high earners
```{r}
# Find the income cutoff for the top 1% earners
top_1_percent_cutoff <- quantile(app_train$AMT_INCOME_TOTAL, 0.99, na.rm = TRUE)

cat("Top 1% Income Cutoff:", top_1_percent_cutoff)

# View the top 1% income earners, ordered by highest income
top_1_percent <- app_train %>%
  filter(AMT_INCOME_TOTAL > top_1_percent_cutoff) %>%
  arrange(desc(AMT_INCOME_TOTAL))  # Order by income in descending order

# Print the top 1% income earners ordered by income
print(top_1_percent)

sum(app_train$AMT_INCOME_TOTAL > 3000000)


```



```{r}
# Check the default rates for top 0.1% income earners
high_income_default <- top_1_percent %>%
  group_by(TARGET) %>%
  summarise(count = n())

print(high_income_default)

high_income_data <- app_train[app_train$AMT_INCOME_TOTAL > 2000000, ]

table(high_income_data$TARGET)

# make two million the cut off for removing outlines 
app_train <- app_train[app_train$AMT_INCOME_TOTAL <= 2000000, ]


```
given there are earners in the top 1 percent that do default it is worth keeping them in the dataset but we will drop the few crazy outliers that are skewing the data. 



```{r}

app_train |>
  arrange(desc(AMT_INCOME_TOTAL))


ids_to_remove <- c(114967,336147,385674)

app_train <- app_train |>
  filter(!(SK_ID_CURR %in% ids_to_remove))

```





```{r plots for the train data 1}
# boxplot EXT_SOURCE_1 grouped by TARGET 

ggplot(app_train, aes(x = as.factor(TARGET), y = EXT_SOURCE_1)) +
  geom_boxplot(fill = c("coral", "skyblue")) +
  labs(title = "Loan Default Prevelance: External Source 1", x = "Loan Default", y = "EXT_SOURCE_1") +
  theme_minimal()

# boxplot EXT_SOURCE_2 grouped by TARGET 

ggplot(app_train, aes(x = as.factor(TARGET), y = EXT_SOURCE_2)) +
  geom_boxplot(fill = c("coral", "skyblue")) +
  labs(title = "Loan Default Prevelance: External Source 2", x = "Loan Default", y = "EXT_SOURCE_2") +
  theme_minimal()

# boxplot EXT_SOURCE_3 grouped by TARGET 
ggplot(app_train, aes(x = as.factor(TARGET), y = EXT_SOURCE_3)) +
  geom_boxplot(fill = c("coral", "skyblue")) +
  labs(title = "Loan Default Prevelance: External Source 3", x = "Loan Default", y = "EXT_SOURCE_3") +
  theme_minimal()


```
We see higher instances of no loan defaults across all three external source scores, keep in mind however our data is unevenly distributed in terms of the loan default target. The comparison across these three plots do indicate the scores are similar from all three external sources, this could indicate that including all three could cause issues with multicolinery in the model. 


Lets look at another important feature, the age of the client.

```{r plots for the train data 2}

# Analyze the distribution of age in years

ggplot(app_train, aes(x = Age_in_Years, fill = as.factor(TARGET))) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  labs(title = "Age Distribution by Loan Defaulting", x = "Age (Years)", y = "Count") +
  theme_minimal()

```
This plot again highlights the dramatic difference in distribution of data on the target variable. 


Lets continue to look further at some of the more important features. 

```{r plots for the train data 3}

ggplot(app_train, aes(x = AMT_ANNUITY, fill = as.factor(TARGET))) +
  geom_density(alpha = 0.5) +
  labs(title = "Density of AMT_ANNUITY by Loan Default", x = "AMT_ANNUITY", y = "Density") +
  scale_fill_manual(values = c("slategrey", "khaki"), name = "Loan Default") +
  theme_minimal()

```
This plot shows that Amt_Annuity is fairly evenly distributed across the default status of individuals, however it is right skewed and may need to be normalized through log transformations to improve the model. Amt Annuity is the periodic payment amount the client makes over time to repay their loan (includes both the principal and interest).


```{r plots for the train data 4}
ggplot(app_train, aes(x = EXT_SOURCE_2, y = EXT_SOURCE_3, color = as.factor(TARGET))) +
  geom_point(alpha = 0.5) +
  labs(title = "EXT_SOURCE_2 vs EXT_SOURCE_3 by Loan Default Status", x = "EXT_SOURCE_2", y = "EXT_SOURCE_3") +
  scale_color_manual(values = c("red", "green"), name = "Loan Default") +
  theme_minimal()
```
This proves to be a not very useful plot, however it does indicate that there isn't an obvious distinction between the two groups which is helpful to know, again this plot indicates that including both of these variables in the modeling may impact the results due to issues with multicollinetty. 


```{r plots for the train data 5}
ggplot(app_train, aes(x = NAME_CONTRACT_TYPE, fill = as.factor(TARGET))) +
  geom_bar(position = "stack") +
  labs(title = "Distribution of Loan Default by Contract Type", x = "Contract Type", y = "Count") +
  scale_fill_manual(values = c("blue", "orange"), name = "Loan Default") +
  theme_minimal()

# proportion plot to see the relationship better

ggplot(app_train, aes(x = NAME_CONTRACT_TYPE, fill = as.factor(TARGET))) +
  geom_bar(position = "fill") + 
  labs(title = "Proportion of Loan Default by Contract Type", x = "Contract Type", y = "Proportion") +
  scale_fill_manual(values = c("blue", "orange"), name = "Loan Default") +
  theme_minimal()
```
These plots indicate a higher instance of cash loans compared to revolving loans for those who default on the loans, again this plot shows how the poor distribution of the target variable can impact our model. 


```{r plots for the train data 6}
ggplot(app_train, aes(x = CODE_GENDER, fill = as.factor(TARGET))) +
  geom_bar(position = "stack") +
  labs(title = "Distribution Gender on Loan Default", x = "Gender", y = "Count") +
  scale_fill_manual(values = c("lightblue", "lightgreen"), name = "Loan Default") +
  theme_minimal()

ggplot(app_train, aes(x = CODE_GENDER, fill = as.factor(TARGET))) +
  geom_bar(position = "fill") +
  labs(title = "Distribution Gender on Loan Default", x = "Gender", y = "Count") +
  scale_fill_manual(values = c("lightblue", "lightgreen"), name = "Loan Default") +
  theme_minimal()

unique(app_train$CODE_GENDER)
```
We can see there is a higher proportion of females in the data, when we look at the proportions we see that more Males tend to default than Females. 


```{r}
missing_ext_source <- app_train %>%
  filter(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3))

# View the rows where all three EXT_SOURCE variables are missing
print(missing_ext_source)

# Optional: check how many rows have all three EXT_SOURCE variables missing
cat("Number of rows where all EXT_SOURCE variables are missing:", nrow(missing_ext_source), "\n")

app_train$has_external_source <- ifelse(!is.na(app_train$EXT_SOURCE_1)|
                                         !is.na(app_train$EXT_SOURCE_2) | 
                                         !is.na(app_train$EXT_SOURCE_3), 1, 0)


```
```{r}
# Remove rows where all EXT_SOURCE variables are missing
app_train <- app_train |>
  filter(!(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3)))

```

Do we assume the missing value for that person means they have no credit? 


^^^^ EDA that needs to be cleaner

# Modeling


## Split Train data into test and train with 90% in test


```{r}
set.seed(123)

# Create a 10% training data split
train_indices <- createDataPartition(app_train$TARGET, p = 0.1, list = FALSE)

# Split data into 10% train and 90% test
train_data <- app_train[train_indices, ]   # 10% train
test_data  <- app_train[-train_indices, ]  # 90% test

prop.table(table(train_data$TARGET))
prop.table(table(test_data$TARGET))

# Separate features and target for the training set
train_features <- train_data |>
  select(-TARGET)  # Remove the TARGET column
train_target <- train_data$TARGET  # Store only the TARGET column

# Separate features and target for the test set
test_features <- test_data |> select(-TARGET)  # Remove the TARGET column
test_target <- test_data$TARGET  # Store only the TARGET column

head(train_data)

str(train_data$TARGET)

```


## Simple Regression Model

Do a simple logistic model with the train data

```{r}

simple_model_1 <-glm(factor(TARGET) ~ 1, data = train_data, family = binomial)
summary(simple_model_1)

```

The coeff here is the log odds: so the probability would be calculated at: 0.07914653 


```{r}
# Predict on the training set (the same probability for every observation)
predictions_simple <- predict(simple_model_1, newdata = test_features, type = "response")

# Check the first few predictions
head(predictions_simple)

# Convert probabilities to class labels (0 or 1) using a threshold of 0.5
predicted_class_simple <- ifelse(predictions_simple > 0.5, 1, 0)

# Check the first few predicted class labels
head(predicted_class_simple)

# Compute the confusion matrix
confusionMatrix(factor(predicted_class_simple), factor(test_target))

summary(predictions_simple)

accuracy <- mean(predicted_class_simple == test_target)
cat("Accuracy:", accuracy, "\n")

```

This simple model is basically just the majority classifier:

lets look at its aUC ROC

### ROC/AUC

```{r}
# Compute ROC curve and AUC
roc_curve_simple <- roc(test_target, predictions_simple)
auc_value_simple <- auc(roc_curve_simple)

# Print the AUC value
cat("AUC for a simple glm:", auc_value_simple, "\n")

```


## Model with more terms

```{r}

model_with_features2 <- glm(
  TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + CNT_CHILDREN + DAYS_EMPLOYED + Age_in_Years + AMT_ANNUITY,
  data = train_data,  
  family = binomial
)

# Make predictions on the test set using the logistic regression model
predictions_with_features <- predict(model_with_features2, newdata = test_features, type = "response")

# Convert probabilities to class labels (0 or 1) using a threshold of 0.5
predicted_class_with_features <- ifelse(predictions_with_features > 0.5, 1, 0)

confusionMatrix(factor(predicted_class_with_features), factor(test_target))

summary(model_with_features2)

summary(predictions_with_features)

```

### ROC/AUC with more terms
```{r}


# Compute ROC curve and AUC
roc_curve_with_features <- roc(test_target, predictions_with_features)
auc_value_with_features <- auc(roc_curve_with_features)

# Print the AUC value
cat("AUC with Features:", auc_value_with_features, "\n")


coef(model_with_features2)

```


Adding a few of the relevant features we have improved the model already! 
## logistic model with interaction terms: 

Age * Income interaction term 
Income * number of kids

```{r}

# Fit logistic regression model with Age * Income and Income * Family Members interaction terms

model_interactions3 <- glm(
  TARGET ~ Age_in_Years * AMT_INCOME_TOTAL + AMT_INCOME_TOTAL * CNT_CHILDREN + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + CNT_CHILDREN + DAYS_EMPLOYED,
  data = train_data,  
  family = binomial
)

# Make predictions on the test set using the logistic regression model with interactions
predictions_interactions <- predict(model_interactions3, newdata = test_features, type = "response")

# Convert probabilities to class labels (0 or 1) using a threshold of 0.5
predicted_class_interactions <- ifelse(predictions_interactions > 0.5, 1, 0)


confusionMatrix(factor(predicted_class_interactions), factor(test_target))

# Compute ROC curve and AUC
roc_curve_interactions <- roc(test_target, predictions_interactions)
auc_value_interactions <- auc(roc_curve_interactions)

# Print the AUC value
cat("AUC with Interaction Terms:", auc_value_interactions, "\n")


summary(model_interactions3)

```
The model has improved in performance in terms accuracy, but the interaction terms are not statistically significant. 

## AUC plots 

```{r}
# compare the plots for AUC for all models so far: 

# Create ROC plots for each model

# Simple model ROC plot
roc_plot_simple <- ggplot() +
  geom_line(aes(x = 1 - roc_curve_simple$specificities, y = roc_curve_simple$sensitivities), color = "blue") +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = paste("Simple Model ROC (AUC =", round(auc(roc_curve_simple), 3), ")"), x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# Model with terms ROC plot
roc_plot_terms <- ggplot() +
  geom_line(aes(x = 1 - roc_curve_with_features$specificities, y = roc_curve_with_features$sensitivities), color = "green") +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = paste("Model with Terms ROC (AUC =", round(auc(roc_curve_with_features), 3), ")"), x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# Model with interaction terms ROC plot
roc_plot_interactions <- ggplot() +
  geom_line(aes(x = 1 - roc_curve_interactions$specificities, y = roc_curve_interactions$sensitivities), color = "red") +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = paste("Model with Interactions ROC (AUC =", round(auc(roc_curve_interactions), 3), ")"), x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()


grid.arrange(roc_plot_simple, roc_plot_terms, roc_plot_interactions, ncol = 1)

```

well we know there are large outliers in the data that we did not want to remove, so we know logistic regression is not the best route, what should I try next?

Yes, re-investigating the logistic model with different thresholds is a good idea, especially if you're focused on finding a balance between predicting defaults and minimizing false positives (e.g., giving loans to people who won’t default). The default threshold in logistic regression is 0.5, but depending on your business objective, a different threshold may improve model performance.

## Random Forest:

TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + CNT_CHILDREN + DAYS_EMPLOYED + Age_in_Years + AMT_ANNUITY

```{r}

train_data$TARGET <- factor(train_data$TARGET)

# Define the formula, based on the variables in your logistic regression model
formula_rf <- TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 +
                       CNT_CHILDREN + DAYS_EMPLOYED + Age_in_Years + AMT_ANNUITY

# Train the Random Forest model
rf_model <- randomForest(
  formula = formula_rf,
  data = train_data,         # Training dataset
  #ntree = 500,               # Number of trees (can adjust based on computation and performance)
  #mtry = 3,                  # Number of variables to consider for each split (can tune this)
  #importance = TRUE,        
  na.action = na.omit        # Handle missing values if necessary
)

# Print the model summary
print(rf_model)

str(train_data$TARGET)
```


```{r}

# Random Forest Predictions and Model Performance

test_target <- factor(test_target)
rf_predictions <- predict(rf_model, newdata = test_features, type = "response")

confusionMatrix(rf_predictions, test_target)

summary(rf_predictions)
```




## random forest feature importance 

```{r}

# simple imputation of missing data for random forest 

train_df_imputed <- app_train
for (var in names(train_df_imputed)) {
  if (is.numeric(train_df_imputed[[var]])) {
    train_df_imputed[[var]][is.na(train_df_imputed[[var]])] <- median(train_df_imputed[[var]], na.rm = TRUE)
  }
}

# random forest model

rf_model <- randomForest(TARGET ~ ., data = train_df_imputed, importance = TRUE, ntree = 50)

# order the importance from highest to lowest

importance_df <- as.data.frame(importance(rf_model))
importance_df <- importance_df[order(-importance_df$MeanDecreaseGini), ]
print(importance_df)
```


## UP SAMPLING: 

```{r}

train_data$TARGET <- factor(train_data$TARGET)

# Perform upsampling
upsampled_train_data <- upSample(x = train_features, 
                                 y = train_data$TARGET,                                  
                                 yname = "TARGET")                                      

# Check the class balance after upsampling
table(upsampled_train_data$TARGET)
table(test_data$TARGET)

```


### Run glm model 2 on the upsampled data: INCLUDE IN THE SUBMISSION


```{r}
#  model on upsampled data

glm_model_with_features_upsampled <- glm(
  TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + CNT_CHILDREN + DAYS_EMPLOYED + Age_in_Years + AMT_ANNUITY,
  data = upsampled_train_data,  
  family = binomial
)

# predictions on the test set
predictions_with_features_upsampled <- predict(glm_model_with_features_upsampled, newdata = test_features, type = "response")

# convert probabilities to class labels (0 or 1) using a threshold of 0.55
predicted_class_with_features_upsampled <- ifelse(predictions_with_features_upsampled > 0.55, 1, 0)
predicted_class_with_features_upsampled <- factor(predicted_class_with_features_upsampled, levels = c("0", "1"))
levels(predicted_class_with_features_upsampled)



table(Predicted = predicted_class_with_features_upsampled, Actual = test_data$TARGET)

summary(glm_model_with_features_upsampled)

```
Accuracy has not improved on the logistic model with features for the upsampled data


## SVM Modeling START OF NEW SECTION TO ADD TO THE DOCUMENT

### resplit the data at the same seed, SVM requires TARGET to be numeric

```{r}

# resplit the data at the same seed, SVM requires TARGET to be numeric

app_train_imogen <- app_train |>
  filter(!(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3)))

set.seed(123)

# 10 % split
train_indices_imogen <- createDataPartition(app_train_imogen$TARGET, p = 0.1, list = FALSE)
train_data_SVM <- app_train_imogen[train_indices_imogen, ] 
test_data_SVM  <- app_train_imogen[-train_indices_imogen, ] 

prop.table(table(train_data_SVM$TARGET))
prop.table(table(test_data_SVM$TARGET))

# Separate features and target for the training set
train_features_SVM <- train_data_SVM |>
  select(-TARGET)  # Remove the TARGET column
train_target_SVM <- train_data_SVM$TARGET  # Store only the TARGET column

# Separate features and target for the test set
test_features_SVM <- test_data_SVM |> select(-TARGET)  # Remove the TARGET column
test_target_SVM <- test_data_SVM$TARGET  # Store only the TARGET column

head(train_data_SVM)

str(train_data_SVM$TARGET)
str(train_data_SVM$SK_ID_CURR)

any(is.na(train_data_SVM[, -c(1, 2)])) 


```



#### Explore Correlations: ADD THIS SECTION

SVM models do not perform very well on highly correlated data, as our data includes highly correlated terms, the SVM model may not perform well. 

```{r}

# Correlation matrix: 

#requires numeric variables 
str(train_data_SVM$TARGET)
str(train_data_SVM$SK_ID_CURR)

# deal with NA's
numeric_train_no_missing <- train_data_SVM[, -c(1, 2)]
numeric_train_no_missing <- numeric_train_no_missing[, sapply(numeric_train_no_missing, is.numeric)]

corr_matrix <- cor(numeric_train_no_missing, use = "complete.obs")


corr_table <-as.data.frame(as.table(corr_matrix))

# remove the self correlations 

corr_table <- corr_table[corr_table$Var1 != corr_table$Var2, ]

# sort by the absolute value of the correlation
corr_table <- corr_table[order(abs(corr_table$Freq), decreasing = TRUE), ]

#  top 20 correlations using 
kable(head(corr_table, 20), caption = "Top 20 Non-Self Correlations in the Trainin Dataset")


```


### Exploire linear seperablity of the data:ADD THIS SECTION

```{r}

train_data_SVM$TARGET <- as.factor(train_data_SVM$TARGET)

# plot different combinations of variables to explore linear separability

# Income vs Credit amounts plot
ggplot(train_data_SVM, aes(x = AMT_INCOME_TOTAL, y = AMT_CREDIT, color = TARGET)) +
  geom_point(size = 2) +
  labs(title = "Income vs Credit", x = "AMT_INCOME_TOTAL", y = "AMT_CREDIT") +
  theme_minimal()

# days employed vs Credit Score

# days employed is a negative value to indicate the number of days back from the application date one was employed
train_data_SVM$DAYS_EMPLOYED <- abs(train_data_SVM$DAYS_EMPLOYED) # fix the negatives by taking absolute value

ggplot(train_data_SVM, aes(x = DAYS_EMPLOYED, y = EXT_SOURCE_2, color = TARGET)) +
  geom_point(size = 2) +
  labs(title = "Days emplopyed vs Credit Score", x = "DAYS_EMPLOYED", y = "EXT_SOURCE_2") +
  theme_minimal()
```

These plots indicate that there is not a large amount of linear separability in the data, this means that a linear SVM model may not be the best route, and we should explore the performance of a radial model. 

### encode features for modeling: INCLUDE THIS SECTION 

```{r}
## DUMMY ENCODING:

# combine features from both training and test sets (only temp to ensure dummyfying is even across train and test sets)
combined_features <- rbind(train_features_SVM, test_features_SVM)

# dummy encode data
dummies <- dummyVars(~ ., data = combined_features)
combined_features_encoded <- as.data.frame(predict(dummies, newdata = combined_features))

# split back into train and test sets
train_features_encoded <- combined_features_encoded[1:nrow(train_features_SVM), ]
test_features_encoded <- combined_features_encoded[(nrow(train_features_SVM) + 1):nrow(combined_features_encoded), ]

# scale across with the training set
scales <- preProcess(train_features_encoded, method = c("center", "scale"))

# Apply scaling to both the training and test encoded features
train_features_scaled <- predict(scales, train_features_encoded)
test_features_scaled <- predict(scales, test_features_encoded)


```

Using dummies_train for both train_features and test_features ensures that the dummy variables are consistent across both datasets. 


### scale and center : INCLUDE THIS SECTION

```{r}

# Combine train features and target
train_data_combined <- cbind(train_features_scaled, TARGET = train_target_SVM)

# Combine test features and target
test_data_combined <- cbind(test_features_scaled, TARGET = test_target_SVM)

# remove NA's in the combined training data
train_data_clean <- na.omit(train_data_combined)

# remove NAs in the combined test data
test_data_clean <- na.omit(test_data_combined)

# split back into features and target 
train_features_scaled <- train_data_clean[, -ncol(train_data_clean)]  # All columns except last
train_target_SVM <- train_data_clean$TARGET                               # Last column

# split back into features and target 
test_features_scaled <- test_data_clean[, -ncol(test_data_clean)]     # All columns except last
test_target_SVM <- test_data_clean$TARGET  

# make sure the data is the same size and no errors were created in the scaling/centering
nrow(train_features_scaled) == length(train_target_SVM) 
nrow(test_features_scaled) == length(test_target_SVM)


train_data_final_SVM <- cbind(train_features_scaled, TARGET = train_target_SVM)

```

### linear model - DONT include in submission it takes too long to run

```{r}

svm_model <- svm(TARGET ~ ., data = train_data_final_SVM, kernel = "linear", cost = 1)


svm_predictions <- predict(svm_model, test_features_scaled)
print(length(svm_predictions))
print(length(test_target))


svm_predictions_binary <- ifelse(svm_predictions > 0.5, "1", "0")
confusionMatrix(factor(svm_predictions_binary), factor(test_target_SVM))


```

### radial model base - DONT include takes too long to run

```{r}
# Radial model:

# Train an SVM model with a radial kernel
svm_radial <- svm(TARGET ~ ., data = train_data_final_SVM, kernel = "radial", cost = 1, gamma = 0.1)

# Make predictions on the test set
svm_radial_predictions <- predict(svm_radial, test_features_scaled)

radial_predictions_binary <- ifelse(svm_radial_predictions > 0.5, "1", "0")
confusionMatrix(factor(radial_predictions_binary), factor(test_target_SVM))
# still jsut majoirty

```
this did not do any better than majoirty class, and is just predicting 0 for all data points

## Hyper param tuning on base radial model: DONT include takes too long to run

temporarily reduce the data set to find a good parameter

set.seed(123)
sample_indices <- sample(1:nrow(train_data_final), size = 5000)  # Adjust sample size as needed
train_data_sample <- train_data_final[sample_indices, ]


```{r}


# Convert 0 and 1 to factors with appropriate labels for both train and test targets
train_target_tuning <- factor(train_target, levels = c(0, 1), labels = c("No", "Yes"))
test_target_tuning <- factor(test_target, levels = c(0, 1), labels = c("No", "Yes"))

# Re-combine with the features after converting the target
train_data_tuning <- cbind(train_features_scaled, TARGET = train_target_tuning)

set.seed(123)
sample_indices <- sample(1:nrow(train_data_tuning), size = 5000)  # Adjust sample size as needed
train_data_sample <- train_data_tuning[sample_indices, ]


fitControl <- trainControl(
  method = "repeatedcv",          # Repeated cross-validation
  number = 4,                     # 4-fold cross-validation
  repeats = 2,                    # Repeat cross-validation twice
  summaryFunction = twoClassSummary,  # Use ROC for evaluation
  classProbs = TRUE               # Needed for ROC metric
)

grid <- expand.grid(
  sigma = c(0.01, 0.05),         # Gamma equivalents for the RBF kernel
  C = c(0.05, 0.75, 1, 1.5, 2)   # Different values of cost
)


# Train the radial SVM with hyperparameter tuning
svmFitRadial <- train(
  TARGET ~ ., data = train_data_sample,
  method = "svmRadial",
  trControl = fitControl,
  metric = "ROC",
  tuneGrid = grid,
  verbose = FALSE
)


```


```{r}

# View the best model parameters
print(svmFitRadial)  # Shows the best values for C and sigma

# Plot ROC with different values of C and sigma
plot(svmFitRadial)

svm_radial_predictions_best <- predict(svmFitRadial, test_features_scaled)

test_target <- factor(test_target, levels = c(0, 1), labels = c("No", "Yes"))


confusionMatrix(svm_radial_predictions_best, test_target)

```



This model is likley performing poortly due to the class imbalance, and the outliers and missing data 

- there is a ton of data, we already took it down to a smaller sample of our training data, I started with 2500 and then went to 5000, and saw improvment in the AUC so i think that is affecting the results, however, including more forces the model to run for a really long time


### SVM Radial model on just the set of features - DO INCLUDE
Model a radial SVM on the set of features we used for the glmmodel 2

```{r}
# Select only the desired features in the training and test sets
selected_features <- c("AMT_INCOME_TOTAL", "AMT_CREDIT", "EXT_SOURCE_1","EXT_SOURCE_2", "EXT_SOURCE_3","CNT_CHILDREN","DAYS_EMPLOYED", "Age_in_Years","AMT_ANNUITY" )  # Specify your chosen features

train_features_selected <- train_features_scaled[, selected_features]
test_features_selected <- test_features_scaled[, selected_features]

train_data_selected <- cbind(train_features_selected, TARGET = train_target_SVM)

head(train_data_selected)
```

#### Hyper parameter Tuning for the radial model on select features : DO INCLUDE

```{r}

train_data_selected$TARGET <- factor(train_data_selected$TARGET, levels = c(0, 1), labels = c("No", "Yes"))

# Define your trainControl and grid if you haven’t already
fitControl <- trainControl(
  method = "repeatedcv",
  number = 4,
  repeats = 2,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)

grid <- expand.grid(sigma = c(0.01, 0.05), C = c(0.05, 0.75, 1, 1.5, 2))

# Train the radial SVM model using only the selected features
svmFitRadial_selected <- train(
  TARGET ~ ., data = train_data_selected,
  method = "svmRadial",
  trControl = fitControl,
  metric = "ROC",
  tuneGrid = grid,
  verbose = FALSE
)

```


####  model perfomance for radial model with features: DO INCLUDE

```{r}

print(svmFitRadial_selected)

# Make predictions on the test set with only the selected features

svm_radial_predictions_selected <- predict(svmFitRadial_selected, test_features_selected)

test_target_SVM <- factor(test_target_SVM, levels = c(0, 1), labels = c("No", "Yes"))

# Evaluate the model
confusionMatrix(svm_radial_predictions_selected, test_target_SVM)


```

model is not producing any true positives, it suggests that it might be overly biased toward predicting the negative class. This could happen due to class imbalance, poor hyper-parameter tuning, or the model simply not finding enough distinguishing features for the positive class. 

We could look at running the model on the upsampled data, or just sticking with improving the RandomForest model, as those models tend to handle class imbalance and outliers better, 