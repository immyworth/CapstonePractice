---
title: "EDA"
author: "Imogen Holdsworth" 
date: "2024-09-20"
output: 
  html_document: 
    code_folding: hide  
    number_sections: no
    toc: yes
editor_options: 
  chunk_output_type: inline
---

# Introduction: 

Home Credit aims to promote financial inclusion by offering accessible loans to customers with limited or no formal credit history. To achieve this, Home Credit must accurately assess default risk, ensuring that loans are responsibly distributed without increasing risk exposure. This project seeks to leverage client data—such as demographics, loan history, and behavior—to develop a model that predicts the likelihood of default. The solution will improve risk management, reduce losses, expand the company’s market reach, and enhance customer experience, all while ensuring higher model accuracy measured by the ROC curve.

This analysis includes the following data sources:

The application data (split into test and train- where the test data does not contain the target variable). This includes static data on all the applicants, where each row is one loan. 

The prior application data includes data on the previous applications for Home Credit loans of clients who have loans in the sample, there is one row for each previous application related to loans in the data set.

## Data and package load

```{r setup, include=FALSE}

knitr::opts_chunk$set(warning = FALSE)

pacman::p_load(ggplot2, dplyr, skimr, caret, tidyverse, knitr, janitor, randomForest, gridExtra, corrplot)

app_train <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_train.csv")

app_test <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_test.csv")

prior_app <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/previous_application.csv")

bureau <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/bureau.csv")

dd <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/HomeCredit_columns_description.csv")
```

# Application Data:

The following section will explore the application data, for both the test and train groups. The goal is to discover any important relationships or patterns that exist in the data, and understand how they impact loan repayment by clients. We will work to begin preliminary identification of potential predictors of clients loan repayment ability, and set the foundation for our analysis.  


## Data Overview

```{r data overview train application data}
# Basic data overview

# what does the data frame look like?
head(app_train)

# Data Structure
str(app_train)

# Data Summary
summary(app_train)

dim(app_train)
```

The data set on current applicants split into the train group consists of 307,511 observations across 123 variables.

The problem here is binary classification, where we want to predict 1 (if we think our applicant is a defaulter), and 0 (if we think our applicant is not a defaulter). 

In thinking about our models performance we will likely want to avoid miss-classifications if possible as it will be a costly mistake. 


## Missing Data

Now we will begin to identify where we have missing instances of data in the data set.


```{r find missing data in the application train dataset}

# locate only columns with missing values

missing_values <- colSums(is.na(app_train))

missing_values <- missing_values[missing_values > 0]


# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```

Here we can see that there is a ton of missing data, 38 of our 123 variables have over 50% of the data missing. The top few variables with missing data include, the Age of the client's car,EXT_SOURCE_1(Normalized score from an external data source), information about the building where the client lives (APARTMENTS_AVG, APARTMENTS_MODE, APARTMENTS_MEDI), information about the basement size of the clients building(BASEMENTAREA_AVG, BASEMENTAREA_MODE, BASEMENTAREA_MEDI), information on the buildings use (YEARS_BEGINEXPLUATATION_AVG, YEARS_BEGINEXPLUATATION_MODE, YEARS_BEGINEXPLUATATION_MEDI), data on the construction dates of the building (YEARS_BUILD_AVG, YEARS_BUILD_MODE, YEARS_BUILD_MEDI), and more. We will need to determine with more than 50% of the data missing, whether or not we drop the variables completely  or impute using the data that we have. If the variables are not very valuable to our model it is likely we will see better success with dropping them. Further in our analysis we will confront these data issues as we learn more about the relationships that exist in the data set.  

Notes:
    
  When we drop the data numeric data should be imputed with the median, or with KNN/regression if there's a lot of missing data.
  For categorical variables we should use the mode for imputation. 
  
  
```{r}
missing_values <- colSums(is.na(app_test))

missing_values <- missing_values[missing_values > 0]


# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_test)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```

  
Drop the variables containing a large amount of missing data, drop variables with more than 45% of data missing, except Ext Source 1

```{r}

app_train <- app_train %>% select(-c("COMMONAREA_AVG", "COMMONAREA_MODE", "COMMONAREA_MEDI", "NONLIVINGAPARTMENTS_AVG", "NONLIVINGAPARTMENTS_MODE","NONLIVINGAPARTMENTS_MEDI","LIVINGAPARTMENTS_AVG","LIVINGAPARTMENTS_MODE","LIVINGAPARTMENTS_MEDI", "FLOORSMIN_AVG","FLOORSMIN_MODE","FLOORSMIN_MEDI", "YEARS_BUILD_AVG","YEARS_BUILD_MODE","YEARS_BUILD_MEDI","OWN_CAR_AGE","LANDAREA_AVG","LANDAREA_MODE","LANDAREA_MEDI","BASEMENTAREA_AVG","BASEMENTAREA_MODE","BASEMENTAREA_MEDI","NONLIVINGAREA_AVG", "NONLIVINGAREA_MODE", "NONLIVINGAREA_MEDI","ELEVATORS_AVG","ELEVATORS_MODE","ELEVATORS_MEDI", "APARTMENTS_AVG", "APARTMENTS_MODE","APARTMENTS_MEDI", "ENTRANCES_AVG","ENTRANCES_MODE","ENTRANCES_MEDI", "LIVINGAREA_AVG","LIVINGAREA_MODE","LIVINGAREA_MEDI","FLOORSMAX_AVG","FLOORSMAX_MODE","FLOORSMAX_MEDI","YEARS_BEGINEXPLUATATION_AVG","YEARS_BEGINEXPLUATATION_MODE", "YEARS_BEGINEXPLUATATION_MEDI","TOTALAREA_MODE"))

app_test <- app_test %>% select(-c("COMMONAREA_AVG", "COMMONAREA_MODE", "COMMONAREA_MEDI", "NONLIVINGAPARTMENTS_AVG", "NONLIVINGAPARTMENTS_MODE","NONLIVINGAPARTMENTS_MEDI","LIVINGAPARTMENTS_AVG","LIVINGAPARTMENTS_MODE","LIVINGAPARTMENTS_MEDI", "FLOORSMIN_AVG","FLOORSMIN_MODE","FLOORSMIN_MEDI", "YEARS_BUILD_AVG","YEARS_BUILD_MODE","YEARS_BUILD_MEDI","OWN_CAR_AGE","LANDAREA_AVG","LANDAREA_MODE","LANDAREA_MEDI","BASEMENTAREA_AVG","BASEMENTAREA_MODE","BASEMENTAREA_MEDI","NONLIVINGAREA_AVG", "NONLIVINGAREA_MODE", "NONLIVINGAREA_MEDI","ELEVATORS_AVG","ELEVATORS_MODE","ELEVATORS_MEDI", "APARTMENTS_AVG", "APARTMENTS_MODE","APARTMENTS_MEDI", "ENTRANCES_AVG","ENTRANCES_MODE","ENTRANCES_MEDI", "LIVINGAREA_AVG","LIVINGAREA_MODE","LIVINGAREA_MEDI","FLOORSMAX_AVG","FLOORSMAX_MODE","FLOORSMAX_MEDI","YEARS_BEGINEXPLUATATION_AVG","YEARS_BEGINEXPLUATATION_MODE", "YEARS_BEGINEXPLUATATION_MEDI","TOTALAREA_MODE"))

```

Re inspect the amount of missing data:

```{r}
# locate only columns with missing values

missing_values <- colSums(is.na(app_train))

missing_values <- missing_values[missing_values > 0]

# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)

```


Model the distribution of the missing variables to see if we should impute with mean or median:

```{r, warning = FALSE}
# AMT_REQ_CREDIT_BUREAU_HOUR: This variable captures how many times a credit bureau has been contacted about the client’s credit history within a one-hour window

# Create individual plots for each variable
p1 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_HOUR)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_HOUR")

p2 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_DAY)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_DAY")

p3 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_WEEK)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_WEEK")

p4 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_MON)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_MON")

p5 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_QRT)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_QRT")

p6 <- ggplot(app_train, aes(x = AMT_REQ_CREDIT_BUREAU_YEAR)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_YEAR")

# Combine the plots in a grid
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)



```

This helps us understand a few things, there seems to be some outlier in AMT_REQ_CREDIT_BUREAU_QRT, and also all the values are categorigcal, only taking whole values between 0 and at most 200 - except AMT_REQ_CREDIT_BUREAU_DAY which does take half values and is the number of enquiries to the Credit Beura about that client in the last day. We can use the mode to impute these missing data for these 6 variables. 

- These variables are all the same measure just over differnt periods of time, its not likley we would want to keep them all in one model but we can determine which ones to drop later on. 

```{r}
# Impute the above variables for the missing values with the mode of 0

credit_bureau_vars <- c("AMT_REQ_CREDIT_BUREAU_HOUR", "AMT_REQ_CREDIT_BUREAU_DAY", 
                        "AMT_REQ_CREDIT_BUREAU_WEEK", "AMT_REQ_CREDIT_BUREAU_MON", 
                        "AMT_REQ_CREDIT_BUREAU_QRT", "AMT_REQ_CREDIT_BUREAU_YEAR")

# Loop through each variable and replace missing values with the mode (0)
for (var in credit_bureau_vars) {
  app_train[[var]][is.na(app_train[[var]])] <- 0
}

# perform the same imputation across the test set. 

for (var in credit_bureau_vars) {
  app_test[[var]][is.na(app_test[[var]])] <- 0
}

```

Now we are left with: 
OBS_30_CNT_SOCIAL_CIRCLE and OBS_60_CNT_SOCIAL_CIRCLE (Observations):
These variables represent how many people in the client's social circle have been observed with overdue payments (30 or 60 days past due).
Essentially, these variables measure the number of people in the client's social network who are being tracked or monitored regarding their credit status (30 or 60 days overdue).
DEF_30_CNT_SOCIAL_CIRCLE and DEF_60_CNT_SOCIAL_CIRCLE (Defaults):
These variables represent how many people in the client's social circle have actually defaulted on payments (i.e., 30 or 60 days overdue and marked as a default).
So, these are the number of people who not only have overdue payments but are officially classified as being in default for 30 or 60 days.

These could be valauble indicators in our model, lets take a look at the data distribution and see how we can impute the missing values:

```{r}
# Create individual plots for each variable

p7 <- ggplot(app_train, aes(x = OBS_30_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "OBS_30_CNT_SOCIAL_CIRCLE")

p8 <- ggplot(app_train, aes(x = OBS_60_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "OBS_60_CNT_SOCIAL_CIRCLE")

p9 <- ggplot(app_train, aes(x = DEF_30_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DEF_30_CNT_SOCIAL_CIRCLE")

p10 <- ggplot(app_train, aes(x = DEF_60_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DEF_60_CNT_SOCIAL_CIRCLE")


# Combine the plots in a grid
grid.arrange(p7, p8, p9, p10, ncol = 2)
```
We can see that this data is heavily right skewed, we will also use the mode here to impute the missing values in order to not alter the distribution of the data. We can also keep in mind for future analysis this data is heavily skewed and contains some outliers. 


```{r}
# impute the missing data for the variables above:


social_bureau_vars <- c("OBS_30_CNT_SOCIAL_CIRCLE", "OBS_60_CNT_SOCIAL_CIRCLE", 
                        "DEF_30_CNT_SOCIAL_CIRCLE", "DEF_60_CNT_SOCIAL_CIRCLE")

# Loop through each variable and replace missing values with the mode (0)
for (var in social_bureau_vars) {
  app_train[[var]][is.na(app_train[[var]])] <- 0
}

# perform the same imputation across the test set. 

for (var in social_bureau_vars) {
  app_test[[var]][is.na(app_test[[var]])] <- 0
}


```

Now we are left with: 

1. AMT_GOODS_PRICE (278 missing values):
Description: This variable represents the price of the goods that the client is asking for a loan to purchase (for consumer loans). It reflects the amount the loan is intended to cover.
Recommendation:
If the missing values are few (278 out of a large dataset), you could consider imputing the missing values with the median of the variable, as the price might vary significantly based on the goods purchased.
If you have domain knowledge, you could also use the mean or a default value depending on the typical goods price in your dataset.
2. AMT_ANNUITY (12 missing values):
Description: This represents the loan annuity, i.e., the amount the client will pay in periodic installments to repay the loan (principal + interest).
Recommendation: Since there are only 12 missing values, using the median or mean annuity from the training data would be appropriate. The median is generally safer if the data is skewed, as it won't be influenced by outliers.
3. CNT_FAM_MEMBERS (2 missing values):
Description: This variable represents the number of family members the client has.
Recommendation: With only 2 missing values, you could:
Impute with the mode (most common family size) if it is known, as family size tends to be a categorical variable with common values (e.g., 2, 3, 4).
Alternatively, you could impute with the median, especially if you find that most clients have a similar number of family members.
4. DAYS_LAST_PHONE_CHANGE (1 missing value):
Description: This variable captures how many days before the loan application the client changed their phone number.
Recommendation: Since there's only 1 missing value, you could simply impute it with the median value of the existing data. Given that this is a continuous variable representing time, the median is a safe option to prevent outliers from affecting the imputation.
```{r}

p11 <- ggplot(app_train, aes(x = AMT_GOODS_PRICE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_GOODS_PRICE")

p12 <- ggplot(app_train, aes(x = AMT_ANNUITY)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_ANNUITY")
p13 <- ggplot(app_train, aes(x = CNT_FAM_MEMBERS)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "CNT_FAM_MEMBERS")

p14 <- ggplot(app_train, aes(x = DAYS_LAST_PHONE_CHANGE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DAYS_LAST_PHONE_CHANGE")


# Combine the plots in a grid
grid.arrange(p11,p12,p13,p14, ncol = 2)
summary(app_train$DAYS_LAST_PHONE_CHANGE)
```

lets use the mode for the cnt fam members, median for the amt goods price and amt annuity and days last phone cahnge

```{r}

# Impute AMT_GOODS_PRICE with median

app_train$AMT_GOODS_PRICE[is.na(app_train$AMT_GOODS_PRICE)] <- median(app_train$AMT_GOODS_PRICE, na.rm = TRUE)

# Impute AMT_ANNUITY with median

app_train$AMT_ANNUITY[is.na(app_train$AMT_ANNUITY)] <- median(app_train$AMT_ANNUITY, na.rm = TRUE)

# Impute CNT_FAM_MEMBERS with mode (most common value)

mode_fam_members <- as.numeric(names(sort(table(app_train$CNT_FAM_MEMBERS), decreasing = TRUE)[1]))
app_train$CNT_FAM_MEMBERS[is.na(app_train$CNT_FAM_MEMBERS)] <- mode_fam_members

# Impute DAYS_LAST_PHONE_CHANGE with median

app_train$DAYS_LAST_PHONE_CHANGE[is.na(app_train$DAYS_LAST_PHONE_CHANGE)] <- median(app_train$DAYS_LAST_PHONE_CHANGE, na.rm = TRUE)


# Do the same for the test data:

app_test$AMT_GOODS_PRICE[is.na(app_test$AMT_GOODS_PRICE)] <- median(app_test$AMT_GOODS_PRICE, na.rm = TRUE)

app_test$AMT_ANNUITY[is.na(app_test$AMT_ANNUITY)] <- median(app_test$AMT_ANNUITY, na.rm = TRUE)

mode_fam_members_test <- as.numeric(names(sort(table(app_test$CNT_FAM_MEMBERS), decreasing = TRUE)[1]))
app_test$CNT_FAM_MEMBERS[is.na(app_test$CNT_FAM_MEMBERS)] <- mode_fam_members_test

app_test$DAYS_LAST_PHONE_CHANGE[is.na(app_test$DAYS_LAST_PHONE_CHANGE)] <- median(app_test$DAYS_LAST_PHONE_CHANGE, na.rm = TRUE)
```


Lets re run the missing data list:

```{r}
missing_values <- colSums(is.na(app_train))

missing_values <- missing_values[missing_values > 0]

# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(app_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```
Great! the only missing values we have now are just the external sources, we will deal with those soon

Before we begin some visualizations lets reformat age in the train and test data to make sure that it is in years instead of days for better understanding.

```{r fix age}

app_train$Age_in_Years <- -app_train$DAYS_BIRTH / 365
app_test$Age_in_Years <- -app_test$DAYS_BIRTH / 365

app_train <- app_train %>% select(-DAYS_BIRTH)
app_test <- app_test %>% select(-DAYS_BIRTH)
summary(app_train$Age_in_Years)
head(app_train)
```

```{r}
categorical_vars <- app_train %>% select(where(is.character))

# Check for missing values in categorical variables
missing_values_categorical <- sapply(categorical_vars, function(x) sum(is.na(x)))

# Create a data frame to display the results
 data.frame(
  Variable = names(missing_values_categorical),
  MissingCount = missing_values_categorical
)

 unique_values_list <- sapply(categorical_vars, unique)

# Convert the list to a more readable format, collapsing the unique values
data.frame(
  Variable = names(unique_values_list),
  Unique_Values = sapply(unique_values_list, function(x) paste(x, collapse = ", ")),
  stringsAsFactors = FALSE
)

 
 
app_train<- app_train %>% filter(CODE_GENDER != "XNA")
app_train<- app_train %>% filter(ORGANIZATION_TYPE != "XNA")

# Check to make sure XNA is removed
unique(app_train$CODE_GENDER)
unique(app_train$ORGANIZATION_TYPE)

```



Distribution of the target variable:
```{r}
app_train |>
  group_by(TARGET) |>
  summarise(Count = n()) |>
  mutate(Proportion = Count / sum(Count))

ggplot(app_train, aes(x = as.factor(TARGET))) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Target Variable (Loan Default)", x = "Loan Default (1 = Yes, 0 = No)", y = "Count")

```
The data for the Target in the training set is highly skewed, in our case we want to aviod giving a loan to someone who will default, but at the same time not be overly restrictive as we want to contineu our companies mission of loan inclusivity. We may want to consider upsampling the data as it will allow us to keep all the data, and it would help increase the sensitivity to defaults. But we can investigate both and see what kind of affects it has on the modeling. 


Our data is very imbalanced in regards to the target variable, this will impact our analysis and performance grading of the model. Here the majority class is no loan defaults with a proportion of 91.92%, our models will have to perform better than that in order to beat a majority class model. 

clean data with facorizing needed variables

```{r, include = FALSE}
summary(app_train)

character_vars_subset <- app_train %>% select_if(is.character)

# View the structure of only the character variables
str(character_vars_subset)


# Create a list of character variables

character_vars <- c("NAME_CONTRACT_TYPE",
                    "CODE_GENDER",
                    "FLAG_OWN_CAR",
                    "FLAG_OWN_REALTY" ,
                    "NAME_TYPE_SUITE",
                    "NAME_INCOME_TYPE",
                    "NAME_EDUCATION_TYPE", 
                     "NAME_FAMILY_STATUS",
                    "NAME_HOUSING_TYPE",
                    "OCCUPATION_TYPE",
                    "WEEKDAY_APPR_PROCESS_START",
                    "ORGANIZATION_TYPE",
                    "FONDKAPREMONT_MODE",
                    "HOUSETYPE_MODE",
                    "WALLSMATERIAL_MODE",
                    "EMERGENCYSTATE_MODE")
                    

app_train[character_vars] <- lapply(app_train[character_vars], as.factor)

```


  
```{r}
# Investigae integer values that might need to be factored
integer_vars <- names(app_train)[sapply(app_train, is.integer)]

# Print integer variable names
print(integer_vars)

# unique(app_train$) 


integer_vars_subset <- app_train %>% select_if(is.integer)

# View the structure of only the integer variables
str(integer_vars_subset)

  
 
 integer_vars_to_factor <- c("FLAG_MOBIL",
                             "FLAG_EMP_PHONE",
                             "FLAG_WORK_PHONE",
                             "FLAG_CONT_MOBILE",
                             "FLAG_PHONE",
                             "FLAG_EMAIL",
                             "REG_REGION_NOT_LIVE_REGION",
                             "REG_REGION_NOT_WORK_REGION",
                            "LIVE_REGION_NOT_WORK_REGION",
                           "REG_CITY_NOT_LIVE_CITY",
                          "LIVE_CITY_NOT_WORK_CITY",
                          "FLAG_DOCUMENT_2",
                          "FLAG_DOCUMENT_3",
                          "FLAG_DOCUMENT_4",
                          "FLAG_DOCUMENT_5",
                          "FLAG_DOCUMENT_6",
                          "FLAG_DOCUMENT_7",
                          "FLAG_DOCUMENT_8",
                          "FLAG_DOCUMENT_9",
                          "FLAG_DOCUMENT_10",
                          "FLAG_DOCUMENT_11",
                          "FLAG_DOCUMENT_12",
                          "FLAG_DOCUMENT_13",
                          "FLAG_DOCUMENT_14",
                          "FLAG_DOCUMENT_15",
                          "FLAG_DOCUMENT_16",
                          "FLAG_DOCUMENT_17",
                          "FLAG_DOCUMENT_18",
                          "FLAG_DOCUMENT_19",
                          "FLAG_DOCUMENT_20",
                          "FLAG_DOCUMENT_21",
                          "REG_CITY_NOT_WORK_CITY")
 
app_train[integer_vars_to_factor] <- lapply(app_train[integer_vars_to_factor], as.factor)

app_train$REGION_RATING_CLIENT <- factor(app_train$REGION_RATING_CLIENT, ordered = TRUE)
app_train$REGION_RATING_CLIENT_W_CITY <- factor(app_train$REGION_RATING_CLIENT_W_CITY, ordered = TRUE)


str(app_train)


```
  
AMT_REQ_CREDIT_BUREAU_HOUR: Number of credit bureau inquiries in the last hour.
AMT_REQ_CREDIT_BUREAU_DAY: Number of credit bureau inquiries in the last day.
AMT_REQ_CREDIT_BUREAU_WEEK: Number of credit bureau inquiries in the last week.
AMT_REQ_CREDIT_BUREAU_MON: Number of credit bureau inquiries in the last month.
AMT_REQ_CREDIT_BUREAU_QRT: Number of credit bureau inquiries in the last quarter.
AMT_REQ_CREDIT_BUREAU_YEAR: Number of credit bureau inquiries in the last year.
These variables represent count data—discrete numeric values reflecting the number of credit inquiries over different timeframes.





Investigate the relationships that exist in our current dataset:

# plot different combinations of variables to explore linear separability


  
  # Box plot for income vs loan default
ggplot(app_train, aes(x = as.factor(TARGET), y = AMT_INCOME_TOTAL)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Income vs Loan Default", x = "Loan Default (0 = No, 1 = Yes)", y = "Income") +
  theme_minimal()
  
  
  ggplot(app_train, aes(x = Age_in_Years , y = AMT_CREDIT , color = TARGET)) +
  geom_point(size = 2) +
  labs(title = "AMT_INCOME_TOTAL vs AMT_CREDIT ", x = "AMT_INCOME_TOTAL", y = "AMT_CREDIT ") +
  theme_minimal()


# check correlation

```{r}

# Subset of most important variables
important_vars <- c("AMT_CREDIT", "AMT_ANNUITY", "EXT_SOURCE_2", "DAYS_EMPLOYED", "Age_in_Years", "EXT_SOURCE_3", "EXT_SOURCE_1", "TARGET")

# Correlation matrix for important variables only
correlations_subset <- cor(app_train[important_vars], use = "complete.obs")

# Plot the subset correlation matrix
corrplot(correlations_subset, method = "color", type = "upper", tl.cex = 0.8, tl.srt = 45)

```





```{r}
# investigate Income 

ggplot(app_train, aes(x = as.factor(TARGET), y = AMT_INCOME_TOTAL)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Income vs Loan Default", x = "Loan Default", y = "Income") +
  theme_minimal()

ggplot(app_train, aes(x = "", y = AMT_INCOME_TOTAL)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Box Plot for Income (AMT_INCOME_TOTAL)", y = "Income") +
  theme_minimal()

ggplot(app_train, aes(x = "", y = AMT_CREDIT)) +
  geom_boxplot(fill = "purple") +
  labs(title = "Box Plot for  (AMT_CREDIT)", y = "Amount of Credit") +
  theme_minimal()
ggplot(app_train, aes(x = "", y = AMT_ANNUITY)) +
  geom_boxplot(fill = "brown") +
  labs(title = "Box Plot for (AMT_ANNUITY)", y = "Amount Annuity") +
  theme_minimal()
ggplot(app_train, aes(x = Age_in_Years, fill = as.factor(TARGET))) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  labs(title = "Age Distribution by Loan Defaulting", x = "Age (Years)", y = "Count") +
  theme_minimal()
```



Key Variables to Investigate with Box Plots:

Income (AMT_INCOME_TOTAL)
Loan Amount (AMT_CREDIT)
Annuity ()
Family Size (CNT_FAM_MEMBERS)
Age (Age_in_Years)


The IQR rule for identifying outliers in a box plot states that a data point is an outlier if it is more than 1.5 times the interquartile range (IQR) above the third quartile (Q3) or below the first quartile (Q1):
Low outliers: Below Q1 - 1.5 IQR
High outliers: Above Q3 + 1.5 IQR 



investigate super high earners
```{r}
# Find the income cutoff for the top 1% earners
top_1_percent_cutoff <- quantile(app_train$AMT_INCOME_TOTAL, 0.999, na.rm = TRUE)

cat("Top 1% Income Cutoff:", top_1_percent_cutoff)

# View the top 1% income earners, ordered by highest income
top_1_percent <- app_train %>%
  filter(AMT_INCOME_TOTAL > top_1_percent_cutoff) %>%
  arrange(desc(AMT_INCOME_TOTAL))  # Order by income in descending order

# Print the top 1% income earners ordered by income
print(top_1_percent)

```



```{r}
# Check the default rates for top 0.1% income earners
high_income_default <- top_1_percent %>%
  group_by(TARGET) %>%
  summarise(count = n())

print(high_income_default)
```
given there are earners in the top 1 percent that do default it is worth keeping them in the dataset but we will drop the few crazy outliers that are skewing the data. 



```{r}

app_train |>
  arrange(desc(AMT_INCOME_TOTAL))


ids_to_remove <- c(114967,336147,385674)

app_train <- app_train |>
  filter(!(SK_ID_CURR %in% ids_to_remove))

```





```{r plots for the train data 1}
# boxplot EXT_SOURCE_1 grouped by TARGET 

ggplot(app_train, aes(x = as.factor(TARGET), y = EXT_SOURCE_1)) +
  geom_boxplot(fill = c("coral", "skyblue")) +
  labs(title = "Loan Default Prevelance: External Source 1", x = "Loan Default", y = "EXT_SOURCE_1") +
  theme_minimal()

# boxplot EXT_SOURCE_2 grouped by TARGET 

ggplot(app_train, aes(x = as.factor(TARGET), y = EXT_SOURCE_2)) +
  geom_boxplot(fill = c("coral", "skyblue")) +
  labs(title = "Loan Default Prevelance: External Source 2", x = "Loan Default", y = "EXT_SOURCE_2") +
  theme_minimal()

# boxplot EXT_SOURCE_3 grouped by TARGET 
ggplot(app_train, aes(x = as.factor(TARGET), y = EXT_SOURCE_3)) +
  geom_boxplot(fill = c("coral", "skyblue")) +
  labs(title = "Loan Default Prevelance: External Source 3", x = "Loan Default", y = "EXT_SOURCE_3") +
  theme_minimal()


```
We see higher instances of no loan defaults across all three external source scores, keep in mind however our data is unevenly distributed in terms of the loan default target. The comparison across these three plots do indicate the scores are similar from all three external sources, this could indicate that including all three could cause issues with multicolinery in the model. 


Lets look at another important feature, the age of the client.

```{r plots for the train data 2}

# Analyze the distribution of age in years

ggplot(app_train, aes(x = Age_in_Years, fill = as.factor(TARGET))) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  labs(title = "Age Distribution by Loan Defaulting", x = "Age (Years)", y = "Count") +
  theme_minimal()

```
This plot again highlights the dramatic difference in distribution of data on the target variable. 


Lets continue to look further at some of the more important features. 

```{r plots for the train data 3}

ggplot(app_train, aes(x = AMT_ANNUITY, fill = as.factor(TARGET))) +
  geom_density(alpha = 0.5) +
  labs(title = "Density of AMT_ANNUITY by Loan Default", x = "AMT_ANNUITY", y = "Density") +
  scale_fill_manual(values = c("slategrey", "khaki"), name = "Loan Default") +
  theme_minimal()

```
This plot shows that Amt_Annuity is fairly evenly distributed across the default status of individuals, however it is right skewed and may need to be normalized through log transformations to improve the model. Amt Annuity is the periodic payment amount the client makes over time to repay their loan (includes both the principal and interest).


```{r plots for the train data 4}
ggplot(app_train, aes(x = EXT_SOURCE_2, y = EXT_SOURCE_3, color = as.factor(TARGET))) +
  geom_point(alpha = 0.5) +
  labs(title = "EXT_SOURCE_2 vs EXT_SOURCE_3 by Loan Default Status", x = "EXT_SOURCE_2", y = "EXT_SOURCE_3") +
  scale_color_manual(values = c("red", "green"), name = "Loan Default") +
  theme_minimal()
```
This proves to be a not very useful plot, however it does indicate that there isn't an obvious distinction between the two groups which is helpful to know, again this plot indicates that including both of these variables in the modeling may impact the results due to issues with multicollinetty. 


```{r plots for the train data 5}
ggplot(app_train, aes(x = NAME_CONTRACT_TYPE, fill = as.factor(TARGET))) +
  geom_bar(position = "stack") +
  labs(title = "Distribution of Loan Default by Contract Type", x = "Contract Type", y = "Count") +
  scale_fill_manual(values = c("blue", "orange"), name = "Loan Default") +
  theme_minimal()

# proportion plot to see the relationship better

ggplot(app_train, aes(x = NAME_CONTRACT_TYPE, fill = as.factor(TARGET))) +
  geom_bar(position = "fill") + 
  labs(title = "Proportion of Loan Default by Contract Type", x = "Contract Type", y = "Proportion") +
  scale_fill_manual(values = c("blue", "orange"), name = "Loan Default") +
  theme_minimal()
```
These plots indicate a higher instance of cash loans compared to revolving loans for those who default on the loans, again this plot shows how the poor distribution of the target variable can impact our model. 


```{r plots for the train data 6}
ggplot(app_train, aes(x = CODE_GENDER, fill = as.factor(TARGET))) +
  geom_bar(position = "stack") +
  labs(title = "Distribution Gender on Loan Default", x = "Gender", y = "Count") +
  scale_fill_manual(values = c("lightblue", "lightgreen"), name = "Loan Default") +
  theme_minimal()

ggplot(app_train, aes(x = CODE_GENDER, fill = as.factor(TARGET))) +
  geom_bar(position = "fill") +
  labs(title = "Distribution Gender on Loan Default", x = "Gender", y = "Count") +
  scale_fill_manual(values = c("lightblue", "lightgreen"), name = "Loan Default") +
  theme_minimal()

unique(app_train$CODE_GENDER)
```
We can see there is a higher proportion of females in the data, when we look at the proportions we see that more Males tend to default than Females. 


```{r}
missing_ext_source <- app_train %>%
  filter(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3))

# View the rows where all three EXT_SOURCE variables are missing
print(missing_ext_source)

# Optional: check how many rows have all three EXT_SOURCE variables missing
cat("Number of rows where all EXT_SOURCE variables are missing:", nrow(missing_ext_source), "\n")

app_train$has_external_source <- ifelse(!is.na(train_data$EXT_SOURCE_1)|
                                         !is.na(train_data$EXT_SOURCE_2) | 
                                         !is.na(train_data$EXT_SOURCE_3), 1, 0)


```
```{r}
# Remove rows where all EXT_SOURCE variables are missing
app_train <- app_train |>
  filter(!(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3)))

```

Do we assume the missing value for that person means they have no credit? 


^^^^ EDA that needs to be cleaner

# Modeling


## Split Train data into test and train with 90% in test

```{r}
set.seed(123)

# Create a 10% training data split
train_indices <- createDataPartition(app_train$TARGET, p = 0.1, list = FALSE)

# Split data into 10% train and 90% test
train_data <- app_train[train_indices, ]   # 10% train
test_data  <- app_train[-train_indices, ]  # 90% test

prop.table(table(train_data$TARGET))
prop.table(table(test_data$TARGET))

# Separate features and target for the training set
train_features <- train_data |>
  select(-TARGET)  # Remove the TARGET column
train_target <- train_data$TARGET  # Store only the TARGET column

# Separate features and target for the test set
test_features <- test_data |> select(-TARGET)  # Remove the TARGET column
test_target <- test_data$TARGET  # Store only the TARGET column

head(train_data)

```


Do a simple logistic model with the train data

```{r}

simple_model_1 <-glm(TARGET ~ 1, data = train_data, family = binomial)

```
The coeff here is the log odds: so the probability would be calculated at: 0.07914653 

```{r}
# Predict on the training set (the same probability for every observation)
predictions_simple <- predict(simple_model_1, newdata = test_features, type = "response")

# Check the first few predictions
head(predictions_simple)

# Convert probabilities to class labels (0 or 1) using a threshold of 0.5
predicted_class_simple <- ifelse(predictions_simple > 0.5, 1, 0)

# Check the first few predicted class labels
head(predicted_class_simple)

# Compute the confusion matrix
confusionMatrix(factor(predicted_class_simple), factor(test_target))
```
This simple model is basically just the majoirty classifier:

lets look at its aUC ROC

```{r}
# Compute ROC curve and AUC
roc_curve_simple <- roc(test_target, predictions_simple)
auc_value_simple <- auc(roc_curve_simple)

# Print the AUC value
cat("AUC for a simple glm:", auc_value_simple, "\n")

```


## model with more features 

```{r}
model_with_features2 <- glm(
  TARGET ~ AMT_INCOME_TOTAL + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + CNT_CHILDREN + DAYS_EMPLOYED + Age_in_Years,
  data = train_data,  
  family = binomial
)

# Make predictions on the test set using the logistic regression model
predictions_with_features <- predict(model_with_features2, newdata = test_features, type = "response")

# Convert probabilities to class labels (0 or 1) using a threshold of 0.5
predicted_class_with_features <- ifelse(predictions_with_features > 0.5, 1, 0)

confusionMatrix(factor(predicted_class_with_features), factor(test_target))


```
```{r}
# Load pROC package if needed
library(pROC)

# Compute ROC curve and AUC
roc_curve_with_features <- roc(test_target, predictions_with_features)
auc_value_with_features <- auc(roc_curve_with_features)

# Print the AUC value
cat("AUC with Features:", auc_value_with_features, "\n")


coef(model_with_features2)

```




Adding a few of the relevant features we have improved the model already! 
## logistic model with interaction terms: 

Age * Income interaction term 
Income *family members
```{r}
# Fit logistic regression model with Age * Income and Income * Family Members interaction terms
model_interactions3 <- glm(
  TARGET ~ Age_in_Years * AMT_INCOME_TOTAL + AMT_INCOME_TOTAL * CNT_FAM_MEMBERS + AMT_CREDIT + EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + CNT_CHILDREN + DAYS_EMPLOYED,
  data = train_data,  # Make sure TARGET is recombined with train_features
  family = binomial
)

# Make predictions on the test set using the logistic regression model with interactions
predictions_interactions <- predict(model_interactions3, newdata = test_features, type = "response")

# Convert probabilities to class labels (0 or 1) using a threshold of 0.5
predicted_class_interactions <- ifelse(predictions_interactions > 0.5, 1, 0)


confusionMatrix(factor(predicted_class_interactions), factor(test_target))

# Compute ROC curve and AUC
roc_curve_interactions <- roc(test_target, predictions_interactions)
auc_value_interactions <- auc(roc_curve_interactions)

# Print the AUC value
cat("AUC with Interaction Terms:", auc_value_interactions, "\n")




```
roc_curve_simple
roc_curve_with_features
roc_curve_interactions

```{r}
# compare the plots for AUC for all models so far: 

# Create ROC plots for each model

# Simple model ROC plot
roc_plot_simple <- ggplot() +
  geom_line(aes(x = 1 - roc_curve_simple$specificities, y = roc_curve_simple$sensitivities), color = "blue") +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = paste("Simple Model ROC (AUC =", round(auc(roc_curve_simple), 3), ")"), x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# Model with terms ROC plot
roc_plot_terms <- ggplot() +
  geom_line(aes(x = 1 - roc_curve_with_features$specificities, y = roc_curve_with_features$sensitivities), color = "green") +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = paste("Model with Terms ROC (AUC =", round(auc(roc_curve_with_features), 3), ")"), x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# Model with interaction terms ROC plot
roc_plot_interactions <- ggplot() +
  geom_line(aes(x = 1 - roc_curve_interactions$specificities, y = roc_curve_interactions$sensitivities), color = "red") +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(title = paste("Model with Interactions ROC (AUC =", round(auc(roc_curve_interactions), 3), ")"), x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()


grid.arrange(roc_plot_simple, roc_plot_terms, roc_plot_interactions, ncol = 1)

```




well we know there are large outliers in the data that we did not want to remove, so we know logistic regression is not the best route, what should I try next?



Yes, re-investigating the logistic model with different thresholds is a good idea, especially if you're focused on finding a balance between predicting defaults and minimizing false positives (e.g., giving loans to people who won’t default). The default threshold in logistic regression is 0.5, but depending on your business objective, a different threshold may improve model performance.


```{r}
# Load necessary libraries
library(caret)
library(pROC)
library(ggplot2)

# Function to generate predicted probabilities for a specified model
get_predictions <- function(model, test_features) {
  predict(model, newdata = test_features, type = "response")
}

# Function to evaluate model performance at different thresholds
evaluate_thresholds <- function(model, test_features, test_target) {
  # Get predicted probabilities using the specified model
  predicted_probs <- get_predictions(model, test_features)
  
  # Define a range of thresholds to explore
  thresholds <- seq(0.1, 0.9, by = 0.05)
  
  # Initialize lists to store metrics
  sensitivity_values <- c()
  specificity_values <- c()
  precision_values <- c()
  f1_values <- c()
  accuracy_values <- c()
  
  for (threshold in thresholds) {
    # Classify predictions based on the current threshold
    predicted_class <- ifelse(predicted_probs > threshold, 1, 0)
    
    # Compute confusion matrix
    cm <- confusionMatrix(factor(predicted_class), factor(test_target))
    
    # Store sensitivity (recall), specificity, precision, accuracy, and F1 score
    sensitivity_values <- c(sensitivity_values, cm$byClass['Sensitivity'])
    specificity_values <- c(specificity_values, cm$byClass['Specificity'])
    precision_values <- c(precision_values, cm$byClass['Precision'])
    accuracy_values <- c(accuracy_values, cm$overall['Accuracy'])
    
    # Compute F1 score manually
    precision <- cm$byClass['Precision']
    recall <- cm$byClass['Sensitivity']
    f1_score <- 2 * ((precision * recall) / (precision + recall))
    f1_values <- c(f1_values, f1_score)
  }
  
  # Create a data frame with all the metrics for each threshold
  threshold_results <- data.frame(
    Threshold = thresholds,
    Sensitivity = sensitivity_values,
    Specificity = specificity_values,
    Precision = precision_values,
    F1_Score = f1_values,
    Accuracy = accuracy_values
  )
  
  return(threshold_results)
}

threshold_results_simple <- evaluate_thresholds(simple_model_1, test_features, test_target)

threshold_results_features <- evaluate_thresholds(model_with_features2, test_features, test_target)

threshold_results_interactions <- evaluate_thresholds(model_interactions3, test_features, test_target)


```

```{r}
grid.arrange(
  ggplot(threshold_results_simple, aes(x = Threshold)) +
    geom_line(aes(y = Sensitivity, color = "Sensitivity (Simple)")) +
    geom_line(aes(y = Specificity, color = "Specificity (Simple)")) +
    geom_line(aes(y = F1_Score, color = "F1 Score (Simple)")) +
  labs(title = "Performance Metrics at Different Thresholds for simple model") +
    theme_minimal(),
  
  ggplot(threshold_results_features, aes(x = Threshold)) +
    geom_line(aes(y = Sensitivity, color = "Sensitivity (Features)")) +
    geom_line(aes(y = Specificity, color = "Specificity (Features)")) +
    geom_line(aes(y = F1_Score, color = "F1 Score (Features)")) +
    labs(title = "Performance Metrics at Different Thresholds for model with terms") +
    theme_minimal(),
  
  ggplot(threshold_results_interactions, aes(x = Threshold)) +
    geom_line(aes(y = Sensitivity, color = "Sensitivity (Interactions)")) +
    geom_line(aes(y = Specificity, color = "Specificity (Interactions)")) +
    geom_line(aes(y = F1_Score, color = "F1 Score (Interactions)")) +
    labs(title = "Performance Metrics at Different Thresholds for model with interactions") +
    theme_minimal(),
  
  ncol = 1
)
```

Sensitivity (green line): Measures how well the model catches actual defaults (true positives). A high sensitivity means that your model is identifying most of the defaults, but if it’s too high, it might increase false positives (denying loans to people who wouldn’t default).

Specificity (blue line): Measures how well the model avoids false positives (i.e., predicting someone will default when they won’t). High specificity means fewer people who wouldn’t default are being denied, but if it’s too high, you might miss true positives (defaults).

F1 Score (red line): Balances precision and recall (sensitivity). It helps you balance identifying defaults (recall) while maintaining precision (minimizing false positives). A higher F1 score suggests a better balance.
If catching defaults is your highest priority, you'll want a threshold where sensitivity (green line) remains high, but not at the expense of specificity dropping too much.
If you care more about balancing between catching defaults and minimizing false positives, you should look at the F1 score (red line) and select the threshold where it is maximized.
Select a Threshold Based on Trade-Offs:

High Sensitivity (Threshold < 0.5): This means you are more likely to catch defaults, but the specificity will decrease, meaning you may incorrectly deny loans to people who wouldn’t default.
Balanced Threshold (~ 0.5): In many cases, a threshold around 0.5 balances both sensitivity and specificity. However, given your specific goal (catching defaults but avoiding denials to people who wouldn’t default), you might need to adjust this slightly.
High Specificity (Threshold > 0.5): Specificity improves, but you may miss more defaults (lower sensitivity).
Use the F1 Score:

In all three models, the F1 score (red line) is quite flat and high across different thresholds. This suggests that the model maintains a good balance between precision and recall across different thresholds.
You could choose a threshold where the F1 score is high, but keep in mind how sensitivity and specificity are performing at that threshold.


```{r}
# Find the threshold that maximizes the F1 score
best_threshold_simple <- threshold_results_simple$Threshold[which.max(threshold_results_simple$F1_Score)]
best_threshold_features <- threshold_results_features$Threshold[which.max(threshold_results_features$F1_Score)]
best_threshold_interactions <- threshold_results_interactions$Threshold[which.max(threshold_results_interactions$F1_Score)]

cat("Best Threshold for Simple Model:", best_threshold_simple, "\n")
cat("Best Threshold for Features Model:", best_threshold_features, "\n")
cat("Best Threshold for Interactions Model:", best_threshold_interactions, "\n")
```


Then lets re run the models with the improved thresholds!! 

We know that the logisitc model is probably not the best way to go anyway, we have 


What might we do to improve the models? there are large outliers, there is possivle collinearity with some of the terms lets do some RIDGE Regression:
