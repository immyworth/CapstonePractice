---
title: "Modeling 2.0"
author: "Imogen's Updates"
date: "2024-11-03"
output:
  html_document:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    toc_float:
      position: "left"
execute:
  warning: false
  message: false
---

```{r echo = FALSE, warning=FALSE}
pacman::p_load(caret, psych, rpart, rpart.plot, rJava, RWeka, rminer, matrixStats, knitr, tictoc, tidyverse, dplyr, ggplot2, randomForest, DmWR, pROC, gridExtra, xgboost)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
App_train <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_train.csv") 
#App_test <-  read.csv("home-credit-default-risk//application_test.csv") 

App_train2 <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_train.csv") 
```

## Missing Data

View the amount of missing data to explain our removal process

```{r}
# locate only columns with missing values

missing_values <- colSums(is.na(App_train))

missing_values <- missing_values[missing_values > 0]


# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(App_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```

Remove data that is more than 48% missing which seems to be a "cut off" point for the data that is most missing, the values missing less than 48 percent range from .0003% missing to 19.82% missing. Those can be imputed.


Remove data that is more than 48 percent missing(except external source 1),Remove those with very high income, change birth to be in years, filter out families with more than 15 children

```{r}
#clean up App_Train
App_train <- App_train %>% 
  select(all_of(c("EXT_SOURCE_1")), where(~ mean(is.na(.)) <= 0.48)) %>%   # keep columns with <= 48% missing data and the column EXT_SOURCE_1
  filter(AMT_INCOME_TOTAL <= 2000000) %>% # 42 people default between 1M and 2M
  filter(CNT_CHILDREN <= 15) %>% 
  # Add a new column for age
  mutate(Age = -DAYS_BIRTH / 365)

App_train$TARGET <- as.factor(App_train$TARGET)
```


### Imputation: 

```{r, warning = FALSE}
# AMT_REQ_CREDIT_BUREAU_HOUR: This variable captures how many times a credit bureau has been contacted about the client’s credit history within a one-hour window

# Create individual plots for each variable
p1 <- ggplot(App_train, aes(x = AMT_REQ_CREDIT_BUREAU_HOUR)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_HOUR")

p2 <- ggplot(App_train, aes(x = AMT_REQ_CREDIT_BUREAU_DAY)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_DAY")

p3 <- ggplot(App_train, aes(x = AMT_REQ_CREDIT_BUREAU_WEEK)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_WEEK")

p4 <- ggplot(App_train, aes(x = AMT_REQ_CREDIT_BUREAU_MON)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_MON")

p5 <- ggplot(App_train, aes(x = AMT_REQ_CREDIT_BUREAU_QRT)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_QRT")

p6 <- ggplot(App_train, aes(x = AMT_REQ_CREDIT_BUREAU_YEAR)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_REQ_CREDIT_BUREAU_YEAR")

# Combine the plots in a grid
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)


```

## Imogen add:
```{r}
# Impute the above variables for the missing values with the mode of 0

credit_bureau_vars <- c("AMT_REQ_CREDIT_BUREAU_HOUR", "AMT_REQ_CREDIT_BUREAU_DAY", 
                        "AMT_REQ_CREDIT_BUREAU_WEEK", "AMT_REQ_CREDIT_BUREAU_MON", 
                        "AMT_REQ_CREDIT_BUREAU_QRT", "AMT_REQ_CREDIT_BUREAU_YEAR")

# Loop through each variable and replace missing values with the mode (0)
for (var in credit_bureau_vars) {
  App_train[[var]][is.na(App_train[[var]])] <- 0
}

```


Now we are left with: 
OBS_30_CNT_SOCIAL_CIRCLE and OBS_60_CNT_SOCIAL_CIRCLE (Observations):
These variables represent how many people in the client's social circle have been observed with overdue payments (30 or 60 days past due).

Essentially, these variables measure the number of people in the client's social network who are being tracked or monitored regarding their credit status (30 or 60 days overdue).

DEF_30_CNT_SOCIAL_CIRCLE and DEF_60_CNT_SOCIAL_CIRCLE (Defaults):
These variables represent how many people in the client's social circle have actually defaulted on payments (i.e., 30 or 60 days overdue and marked as a default).

So, these are the number of people who not only have overdue payments but are officially classified as being in default for 30 or 60 days.

These could be valuable indicators in our model, lets take a look at the data distribution and see how we can impute the missing values:

```{r, warning = FALSE}
# Create individual plots for each variable

p7 <- ggplot(App_train, aes(x = OBS_30_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "OBS_30_CNT_SOCIAL_CIRCLE")

p8 <- ggplot(App_train, aes(x = OBS_60_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "OBS_60_CNT_SOCIAL_CIRCLE")

p9 <- ggplot(App_train, aes(x = DEF_30_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DEF_30_CNT_SOCIAL_CIRCLE")

p10 <- ggplot(App_train, aes(x = DEF_60_CNT_SOCIAL_CIRCLE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DEF_60_CNT_SOCIAL_CIRCLE")


# Combine the plots in a grid
grid.arrange(p7, p8, p9, p10, ncol = 2)
```


```{r}
# impute the missing data for the variables above:


social_bureau_vars <- c("OBS_30_CNT_SOCIAL_CIRCLE", "OBS_60_CNT_SOCIAL_CIRCLE", 
                        "DEF_30_CNT_SOCIAL_CIRCLE", "DEF_60_CNT_SOCIAL_CIRCLE")

# Loop through each variable and replace missing values with the mode (0)
for (var in social_bureau_vars) {
  App_train[[var]][is.na(App_train[[var]])] <- 0
}

```


```{r, warning = FALSE}

p11 <- ggplot(App_train, aes(x = AMT_GOODS_PRICE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_GOODS_PRICE")

p12 <- ggplot(App_train, aes(x = AMT_ANNUITY)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "AMT_ANNUITY")
p13 <- ggplot(App_train, aes(x = CNT_FAM_MEMBERS)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "CNT_FAM_MEMBERS")

p14 <- ggplot(App_train, aes(x = DAYS_LAST_PHONE_CHANGE)) + 
  geom_bar(fill = "skyblue", color = "black") + 
  labs(title = "DAYS_LAST_PHONE_CHANGE")


# Combine the plots in a grid
grid.arrange(p11,p12,p13,p14, ncol = 2)
summary(App_train$DAYS_LAST_PHONE_CHANGE)
```


lets use the mode for the count fam members, median for the amt goods price and amt annuity and days last phone change, the visualization for the days last phone change indicates some possible outliers and may require additional fixing. 

```{r}

# Impute AMT_GOODS_PRICE with median

App_train$AMT_GOODS_PRICE[is.na(App_train$AMT_GOODS_PRICE)] <- median(App_train$AMT_GOODS_PRICE, na.rm = TRUE)

# Impute AMT_ANNUITY with median

App_train$AMT_ANNUITY[is.na(App_train$AMT_ANNUITY)] <- median(App_train$AMT_ANNUITY, na.rm = TRUE)

# Impute CNT_FAM_MEMBERS with mode (most common value)

mode_fam_members <- as.numeric(names(sort(table(App_train$CNT_FAM_MEMBERS), decreasing = TRUE)[1]))
App_train$CNT_FAM_MEMBERS[is.na(App_train$CNT_FAM_MEMBERS)] <- mode_fam_members

# Impute DAYS_LAST_PHONE_CHANGE with median

App_train$DAYS_LAST_PHONE_CHANGE[is.na(App_train$DAYS_LAST_PHONE_CHANGE)] <- median(App_train$DAYS_LAST_PHONE_CHANGE, na.rm = TRUE)

```

```{r}
summary(App_train$DAYS_LAST_PHONE_CHANGE) # "How many days before application did client change the phone?"

#  proportion of zero values
 sum(App_train$DAYS_LAST_PHONE_CHANGE == 0) / nrow(App_train)

```
 Was determined that there were no obvious issues, Days last phone change indicates "How many days before application did client change the phone?" only 11 % of the data indicates a phone change of day of the application. 


Lets re run the missing data list:

```{r}
missing_values <- colSums(is.na(App_train))

missing_values <- missing_values[missing_values > 0]

# reformat for percentage of missing data

missing_data_summary <- tibble(
  Variable = names(missing_values),               
  MissingCount = as.numeric(missing_values),      
  MissingPercentage = (as.numeric(missing_values) / nrow(App_train)) * 100  
)

missing_data_summary <- missing_data_summary |>
  arrange(desc(MissingPercentage))

# display the summary table

kable(missing_data_summary)
```
Great now our only missing data is the external sources. 

## Resolve Data Issues
fix the days employed, make it positive days employed and also remove the error of 365243
```{r}
# days employ
table(App_train$'TARGET') # 282642  24823  

sum(App_train$DAYS_EMPLOYED == 365243)

App_train <- App_train[App_train$DAYS_EMPLOYED != 365243, ]

table(App_train$'TARGET') # 230260  21833  clean up resulted in some loss of the positive instance of defaulting

App_train$DAYS_EMPLOYED <- abs(App_train$DAYS_EMPLOYED)

summary(App_train$DAYS_EMPLOYED) # Check the updated column
  
```

Days employed is now cleaned up, and positive.


## Missing External Source 

Now to deal with the missing data on external source:
Lets create an indicator for where applicants are missing data from external source. 

```{r}
# Check how many people have no value across all external sources
App_train <- App_train %>%
  mutate(All_EXT_Missing = ifelse(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3), 1, 0))

# Count the number of people with all external sources missing
total_missing <- sum(App_train$All_EXT_Missing)
cat("Total people with all EXT_SOURCEs missing:", total_missing, "\n")

# Group by TARGET and count how many people have all external sources missing for each TARGET value
missing_by_target <- App_train %>%
  filter(All_EXT_Missing == 1) %>%
  group_by(TARGET) %>%
  summarise(Count = n())

# Display the counts for TARGET = 0 and TARGET = 1
missing_by_target


```
there are 128 defaulters who have NA values across all the external sources

```{r}
summary(App_train$EXT_SOURCE_1)
summary(App_train$EXT_SOURCE_2)
summary(App_train$EXT_SOURCE_3)

```

create missing indicator variables: These indicators may capture predictive signals related to financial exclusion or lack of a credit history.Complete absence of credit score has been captured by All_EXT_Missing 

```{r}

App_train <- App_train %>%
  mutate(
    EXT_SOURCE_1_Missing = ifelse(is.na(EXT_SOURCE_1), 1, 0),
    EXT_SOURCE_2_Missing = ifelse(is.na(EXT_SOURCE_2), 1, 0),
    EXT_SOURCE_3_Missing = ifelse(is.na(EXT_SOURCE_3), 1, 0)
  )

head(App_train)

```


```{r}
# correlation matrix for EXT_SOURCE variables
correlation_matrix <- App_train %>%
  select(EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) %>%
  cor(use = "pairwise.complete.obs")
print(correlation_matrix)
```

```{r}
# default rates by missingness for each EXT_SOURCE
missing_target_summary <- App_train %>%
  mutate(
    All_EXT_Missing = ifelse(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3), 1, 0)
  ) %>%
  group_by(EXT_SOURCE_1_Missing, EXT_SOURCE_2_Missing, EXT_SOURCE_3_Missing, All_EXT_Missing) %>%
  summarise(
    Default_Rate = mean(TARGET),
    Count = n())
print(missing_target_summary)
```

```{r}
App_train %>%
  select(EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) %>%
  pivot_longer(cols = everything(), names_to = "Source", values_to = "Value") %>%
  filter(!is.na(Value)) %>%
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~Source, scales = "free") +
  labs(title = "Distribution of Non-Missing EXT_SOURCE Values", x = "Value", y = "Count")
```


```{r}
# missingness by demographic
missing_by_features <- App_train %>%
  mutate(
    EXT_SOURCE_1_Missing = ifelse(is.na(EXT_SOURCE_1), 1, 0),
    EXT_SOURCE_2_Missing = ifelse(is.na(EXT_SOURCE_2), 1, 0),
    EXT_SOURCE_3_Missing = ifelse(is.na(EXT_SOURCE_3), 1, 0)
  ) %>%
  group_by(EXT_SOURCE_1_Missing, EXT_SOURCE_2_Missing, EXT_SOURCE_3_Missing) %>%
  summarise(
    Avg_Income = mean(AMT_INCOME_TOTAL),
    Avg_Age = mean(-DAYS_BIRTH / 365),
    Default_Rate = mean(TARGET),
    Count = n()
  )
print(missing_by_features)
```

^^ use the above to determine the best route for imputing the NA in ext sources
Correlation Analysis:

The external scores (EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) are weakly correlated (highest correlation is 0.23 between EXT_SOURCE_1 and EXT_SOURCE_2).
Implication: These variables represent largely independent sources of information, so missing values for one cannot be reliably predicted from the others.

Distributions:
The histograms show a well-distributed range of values for all three sources.
No extreme skew or outliers, so median imputation is a reasonable approach if imputation is necessary.

Missing values in all three sources are associated with slightly higher average income and older age 


Use median imputation for missing values because the distributions of all three sources are symmetric and central tendency is clear.

The weak correlation between indicates that this imputation should be ok. 

```{r}
App_train <- App_train %>%
  mutate(
    EXT_SOURCE_1 = ifelse(is.na(EXT_SOURCE_1), median(EXT_SOURCE_1, na.rm = TRUE), EXT_SOURCE_1),
    EXT_SOURCE_2 = ifelse(is.na(EXT_SOURCE_2), median(EXT_SOURCE_2, na.rm = TRUE), EXT_SOURCE_2),
    EXT_SOURCE_3 = ifelse(is.na(EXT_SOURCE_3), median(EXT_SOURCE_3, na.rm = TRUE), EXT_SOURCE_3)
  )
```

```{r}
App_train %>%
  group_by(All_EXT_Missing, TARGET) %>%
  summarise(Count = n(), Default_Rate = mean(TARGET, na.rm = TRUE))
```



## Feature engineering 

```{r include = FALSE}
  
# Credit to income ratio: Basic
summary(App_train$AMT_INCOME_TOTAL)

# create super simple debt to income using credit and income
App_train <- App_train |> 
  mutate(DTI = AMT_CREDIT / AMT_INCOME_TOTAL)

# loan amount to credit score

App_train <- App_train |>
  mutate(
    Loan_to_Credit_Score1 = AMT_CREDIT / EXT_SOURCE_1,
    Loan_to_Credit_Score2 = AMT_CREDIT / EXT_SOURCE_2,
    Loan_to_Credit_Score3 = AMT_CREDIT / EXT_SOURCE_3
  )

# annuity to income :  how much of their income a borrower spends on periodic loan payments ->  high ratio might flag financial stress due to high loan payments

App_train <- App_train |>
  mutate(Annuity_to_Income_Ratio = AMT_ANNUITY / AMT_INCOME_TOTAL)
summary(App_train$Annuity_to_Income_Ratio)

# credit "duration" -> credit/annuity  higher ratio suggests longer repayment terms, which could be riskier

App_train <- App_train |>
  mutate(Credit_Duration_Ratio = AMT_CREDIT / AMT_ANNUITY)


# income/ family members:

App_train <- App_train |>
  mutate(
    Per_Capita_Income = AMT_INCOME_TOTAL / CNT_FAM_MEMBERS)


# loan /family memebers: 

App_train <- App_train |>
  mutate(
    Per_Capita_Loan = AMT_CREDIT / CNT_FAM_MEMBERS)


```


# Task 1

**Set up a training set and a validation set using application_train.csv data set to do cross-validation.  Alternatively you could perform cross-validation using a different framework, such as k-fold cross validation as implemented in modeling packages such as caret or tidymodels or scikit-learn. The model performance that matters, of course, is the estimated performance on the test set as well as the Kaggle score.**

```{r}
head(App_train)
set.seed(123)

#keep at 10% because the data set is large
inTrain <- createDataPartition(App_train$TARGET, p = .1, list = FALSE)


train_set <- App_train[inTrain,]
test_set <- App_train[-inTrain,]
test_target <- test_set$TARGET
head(test_target)
head(train_set)
head(test_set)

# need to remove target from test set:
test_set <- test_set |>
  select(-TARGET)

#test_target <- App_train[-inTrain,2] # this was casusing issues as the train target was not column 2 for me


```


# Task 2

**Identify the performance benchmark established by the majority class classifier.**

```{r}
prop.table(table(App_train$'TARGET'))
```


If we just went off the majority classifier to predict if someone would default, they would not 91.92% of the time. The performance benchmark is 91.92%.

# Task 3
**Fit several different logistic regression models using different predictors. Do interaction terms improve the model?  Compare model performance using not just accuracy but also AUC.**

```{r}
glmModel1 <- glm(TARGET~ 1, family = binomial, data = train_set)

summary(glmModel1)
```

```{r}
prob <- exp(coef(glmModel1)[1]) / (1 + exp(coef(glmModel1)[1]))
prob
```


The odds of the target being one are 0.086. The negative intercept shows that there are low likelihood of the target being 1

```{r}

predictions1 <- predict(glmModel1, newdata = test_set, type = "response")
summary(predictions1)
mean(predictions1)

```
Just using the target gets us to 91.93

```{r}
#head(test_target)
#table(test_target) ensure test target is in 0 and 1 binary
unique(test_target)


roc_curve_model1 <- roc(test_target, predictions1)
auc_value_model1 <- auc(roc_curve_model1)
print(paste("AUC:", auc_value_model1))

plot(roc_curve_model1, main = "ROC Curve for Baseline Model", col = "blue", lwd = 2)


```
An AUC of 0.5 indicates that this model has no discriminatory ability—it is performing no better than random guessing.

Next: add in our features : add in the generated feartures 

next step build a model with predictors, show the value of the created features I made. 
compare


```{r}

summary(predictions1)


predicted_classes <- ifelse(predictions1 > 0.5, 1, 0)

confusionMatrix(factor(predicted_classes), factor(test_target))

unique(predicted_classes)
unique(test_target)

# make sure test_target is a factor with levels 0 and 1
test_target <- factor(test_target, levels = c(0, 1))

# make predicted classes (already all 0s in this case)
predicted_classes <- factor(predicted_classes, levels = c(0, 1))

# confusion matrix
library(caret)
cm <- confusionMatrix(predicted_classes, test_target)
print(cm)

```



```{r}

train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)

train_set$TARGET <- factor(train_set$TARGET, levels = c(0, 1), labels = c("No", "Yes"))
test_target <- factor(test_target, levels = c(0, 1), labels = c("No", "Yes"))

glm_cv <- train(
  TARGET ~ Age + EXT_SOURCE_1 + EXT_SOURCE_2 +  EXT_SOURCE_3 + AMT_INCOME_TOTAL + CNT_CHILDREN + DAYS_EMPLOYED + AMT_CREDIT + AMT_ANNUITY, 
  data = train_set, 
  method = "glm",
  family = binomial,
  trControl = train_control,
  metric = "ROC"
)

# cross-validated AUC
print(glm_cv)

predictions2IG <- predict(glm_cv, newdata = test_set, type = "prob")[, 2]


# ROC curve and AUC calculation 
roc_curve_model2 <- roc(test_target, predictions2IG)
auc_test_2 <- auc(roc_curve_model2)
print(paste("Added glm model with feature AUC:", auc_test_2))

# ROC curve plot
plot(roc_curve_model2, main = "ROC Curve glm with features", col = "blue", lwd = 2)

```
much of an improvement from .5


create another model adding in the engineered features


```{r}
glm_cv2 <- train(
  TARGET ~ Age + EXT_SOURCE_1 + EXT_SOURCE_2 +  EXT_SOURCE_3 + AMT_INCOME_TOTAL + CNT_CHILDREN + DAYS_EMPLOYED + AMT_CREDIT + AMT_ANNUITY + Per_Capita_Income + Credit_Duration_Ratio + Per_Capita_Loan + Loan_to_Credit_Score1 + Loan_to_Credit_Score2 + Loan_to_Credit_Score3 + DTI + EXT_SOURCE_1_Missing + EXT_SOURCE_2_Missing + EXT_SOURCE_3_Missing + All_EXT_Missing, 
  data = train_set, 
  method = "glm",
  family = binomial,
  trControl = train_control,
  metric = "ROC"
)

# cross-validated AUC again
print(glm_cv2)
summary(glm_cv2)

predictions3IG <- predict(glm_cv2, newdata = test_set, type = "prob")[, 2]


# ROC curve and AUC
roc_curve_model3 <- roc(test_target, predictions3IG)
auc_test_3 <- auc(roc_curve_model3)
print(paste("Added glm model with feature AUC:", auc_test_3))

# ROC curve plot
plot(roc_curve_model3, main = "ROC Curve glm with engineered features", col = "blue", lwd = 2)

```

visualize all three glm curves togther

EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + DAYS_EMPLOYED + Loan_to_Credit_Score1 + EXT_SOURCE_1_Missing + EXT_SOURCE_3_Missing are the only significant vairables



```{r}
# first ROC curve
plot(roc_curve_model1, main = "ROC Curve Comparison", col = "red", lwd = 2)

# second ROC curve
plot(roc_curve_model2, col = "blue", lwd = 2, add = TRUE)

# third ROC curve
plot(roc_curve_model3, col = "green", lwd = 2, add = TRUE)

legend("bottomright", 
       legend = c("Baseline Model", "GLM with Features", "GLM with Engineered Features"), 
       col = c("red", "blue", "green"), 
       lwd = 2)
```

compare aucroc

```{r}
# Compare ROC curves (can only do two at a time)
roc.test(roc_curve_model2, roc_curve_model3)

```


```{r}
# Define control for RFE
#rfe_control <- rfeControl(functions = caretFuncs, method = "cv", number = 5)

# Perform RFE
#rfe_results <- rfe(
 # x = train_set[, -which(names(train_set) == "TARGET")],
 # y = train_set$TARGET,
 # sizes = c(5, 10, 15, 20), # Number of predictors to test
  #rfeControl = rfe_control
#)

# View results
#print(rfe_results)
# print(rfe_results$optVariables)
```

^ too computationally expensive, took a long time, we can take a look at the important features after we run the XGBoost model



## Explore Random Forest:

```{r}
library(randomForest)


# Random Forest Model 
rf_model <- randomForest(
  TARGET ~ Age + EXT_SOURCE_1 + EXT_SOURCE_2 +  EXT_SOURCE_3 + AMT_INCOME_TOTAL + CNT_CHILDREN + DAYS_EMPLOYED + AMT_CREDIT + AMT_ANNUITY + Per_Capita_Income + Credit_Duration_Ratio + Per_Capita_Loan + Loan_to_Credit_Score1 + Loan_to_Credit_Score2 + Loan_to_Credit_Score3 + DTI + EXT_SOURCE_1_Missing + EXT_SOURCE_2_Missing + EXT_SOURCE_3_Missing + All_EXT_Missing, 
  data = train_set,
  ntree = 500,       # Number of trees
  mtry = 3,          # Number of features to consider at each split
  importance = TRUE  # To calculate feature importance
)

# predictions in probabilities
rf_predictions <- predict(rf_model, newdata = test_set, type = "prob")[, 2]

# AUC
roc_curve_rf <- roc(test_target, rf_predictions)
auc_rf <- auc(roc_curve_rf)
print(paste("Random Forest AUC:", auc_rf))

# feature importance
varImpPlot(rf_model)

plot(roc_curve_rf, main = "ROC Curve Random Forest added engineered features", col = "blue", lwd = 2)

```

-- Random Forest AUC: 0.712062010209949, has gone down

Features that are important:

Loan_to_Credit_Score2 and Loan_to_Credit_Score3:High contribution to model accuracy.

EXT_SOURCE_2 and EXT_SOURCE_3: External credit scores are highly predictive.

DTI (Debt-to-Income Ratio): simple generated feature that we did with credit to income

Per_Capita_Income and Per_Capita_Loan: additional engineered features


EXT_SOURCE_2: this is the most important feature based on reducing impurity.


The top features across both plots are:

EXT_SOURCE_2: External credit score.
Loan_to_Credit_Score2 and Loan_to_Credit_Score3
DTI: Debt-to-income ratio.
Per_Capita_Income and Per_Capita_Loan
Age: Likely a strong demographic predictor.
Credit_Duration_Ratio: Indicates the relationship between loan duration and creditworthiness - engineered feature.




## Explore XGBoost:

XGBoost often outperforms Random Forest on tabular datasets, especially with careful tuning.

Tune Hyperparameters: Focus on:
max_depth: Maximum depth of trees.
eta: Learning rate 
nrounds: Number of boosting rounds.
subsample: Fraction of data used for each tree.
colsample_bytree: Fraction of features used for each tree.

```{r}
library(xgboost)

# convert character columns to factors
train_set_B <- train_set
test_set_B <- test_set

train_set_B<- train_set_B %>%
  mutate(across(where(is.character), as.factor))

# convert factor columns to numeric
train_set_B <- train_set_B %>%
  mutate(across(where(is.factor), as.numeric))

# convert TARGET to numeric
train_set_B$TARGET <- as.numeric(as.character(train_set_B$TARGET)) - 1

train_set_B <- train_set_B %>%
  select(-SK_ID_CURR)


non_numeric_cols <- names(test_set_B)[!sapply(test_set, is.numeric)]
print(non_numeric_cols)

test_set_B <- test_set_B %>%
  mutate(across(where(is.character), as.factor))

test_set_B <- test_set_B %>% select(-SK_ID_CURR)

# convert factors to numeric
test_set_B <- test_set_B %>%
  mutate(across(where(is.factor), as.numeric))
```


```{r}

# feature names from train and test
train_features <- colnames(train_set_B[, -which(names(train_set_B) == "TARGET")])
test_features <- colnames(test_set_B)

# missing and extra features
missing_in_test <- setdiff(train_features, test_features)
extra_in_test <- setdiff(test_features, train_features)

# missing columns to test set
for (col in missing_in_test) {
  test_set_B[[col]] <- 0  # Add missing columns with default values (e.g., 0)
}

# drop extra columns
test_set_B <- test_set_B[, train_features]


print(colnames(train_set_B[, -which(names(train_set_B) == "TARGET")]))
print(colnames(test_set_B))

all(sapply(train_set_B[, -which(names(train_set_B) == "TARGET")], is.numeric))
all(sapply(test_set_B, is.numeric))

print(missing_in_test)


train_features <- colnames(train_set_B[, -which(names(train_set_B) == "TARGET")])
test_features <- colnames(test_set_B)

# compare feature names
all.equal(train_features, test_features)
```


```{r}

# train
dtrain <- xgb.DMatrix(
  data = as.matrix(train_set_B[, -which(names(train_set_B) == "TARGET")]), 
  label = train_set_B$TARGET
)

# test
dtest <- xgb.DMatrix(data = as.matrix(test_set_B))

# Train XGBoost model
xgb_model <- xgboost(
  data = dtrain,
  max_depth = 6,      # Depth of the tree
  eta = 0.1,          # Learning rate
  nrounds = 500,      # Number of boosting rounds
  objective = "binary:logistic", # Binary classification
  eval_metric = "auc",
  subsample = 0.8,    # Row subsampling
  colsample_bytree = 0.8 # Feature subsampling
)

# predictions
xgb_predictions <- predict(xgb_model, newdata = dtest)

# AUC
roc_curve_xgb <- roc(test_target, xgb_predictions)
auc_xgb <- auc(roc_curve_xgb)
print(paste("XGBoost AUC:", auc_xgb))

plot(roc_curve_xgb, main = "ROC Curve XGBoost", col = "blue", lwd = 2)

```
 


```{r}
importance_matrix <- xgb.importance(feature_names = colnames(train_set_B[, -which(names(train_set_B) == "TARGET")]), model = xgb_model)
xgb.plot.importance(importance_matrix)
```
^ get a better output to include in the presentation. 


Lets improve the model with hyperparameter tunning: 

```{r}##


train_control <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)

xgb_tuned <- train(
  TARGET ~ .,
  data = train_set_B,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = xgb_grid,
  metric = "ROC"
)

```
^^ This took a long time to load, over 45 minutes and still did not finish, so I explored other methods of hyper parameter searching. 


## Random Search Hyper Parameter Tuning

```{r}
train_set_B$TARGET <- as.factor(ifelse(train_set_B$TARGET == 1, "Yes", "No"))


library(caret)
xgb_grid <- expand.grid(
  nrounds = c(100, 200, 500),
  max_depth = c(4, 6, 8),
  eta = c(0.01, 0.05, 0.1),
  gamma = c(0, 1, 5),
  colsample_bytree = c(0.6, 0.8, 1),
  min_child_weight = c(1, 5, 10),
  subsample = c(0.6, 0.8, 1)
)

# grid subset
set.seed(123)
random_grid <- xgb_grid %>%
  sample_n(20)  # 20 combos

# train on grid
xgb_tuned <- train(
  TARGET ~ .,
  data = train_set_B,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = random_grid,
  metric = "ROC"
)


```

using random search to sample 20 random combinations from the hyperparameter grid, to find the "best" one with out taking as long as the earlier method. 




## XGboost: tuned hyperparameters

```{r}
# train ne XGBoost model with tuned parameters
xgb_model_tuned <- xgboost(
  data = dtrain,
  max_depth = 4,            
  eta = 0.05,               
  nrounds = 200,           
  gamma = 1,               
  colsample_bytree = 0.6,  
  min_child_weight = 10,    
  subsample = 0.8,          
  objective = "binary:logistic", 
  eval_metric = "auc"      
)

# predictions 
xgb_predictions_tuned <- predict(xgb_model_tuned, newdata = dtest)

#  AUC
roc_curve_xgb_tuned <- roc(test_target, xgb_predictions_tuned)
auc_xgb_tuned <- auc(roc_curve_xgb_tuned)
print(paste("XGBoost AUC with Tuned Parameters:", auc_xgb_tuned))

#  ROC Curve
plot(roc_curve_xgb_tuned, main = "ROC Curve XGBoost (Tuned Parameters)", col = "blue", lwd = 2)
```

XGBoost AUC with Tuned Parameters: 0.750989307097772

Accuracy to compare with majority classifier. 

```{r}
# threshold, I tried a few versions and .6 seemed to result in the best score, but did not have a major diff from .5
threshold <- 0.6

# probabilities to binary
predicted_classes <- ifelse(xgb_predictions_tuned >= threshold, "Yes", "No")

# target in factor
test_target <- factor(test_target, levels = c("No", "Yes"))

# CM
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_target)
print("Confusion Matrix:")
print(confusion_matrix)

# ACC
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))

# additional scores
precision <- confusion_matrix["Yes", "Yes"] / sum(confusion_matrix["Yes", ])
recall <- confusion_matrix["Yes", "Yes"] / sum(confusion_matrix[, "Yes"])
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Precision:", round(precision, 4)))
print(paste("Recall (Sensitivity):", round(recall, 4)))
print(paste("F1-Score:", round(f1_score, 4)))
prop.table(table(App_train$'TARGET'))


```

## XGBoost: Upsample




```{r}
# TARGET is a factor
str(train_set_upsampled$TARGET)
train_set_B$TARGET <- as.factor(train_set_B$TARGET)


library(caret)

# Upsample (on minorty to give model more familairty with the defaults)
train_set_upsampled <- upSample(
  x = train_set_B[, -which(names(train_set_B) == "TARGET")], 
  y = train_set_B$TARGET,                                   
  yname = "TARGET"                                          
)

```


```{r}

# make TARGET back to numeric for XGBoost

train_set_upsampled$TARGET <- as.numeric(as.character(train_set_upsampled$TARGET)) 

# XGBoost matrix
dtrain_upsampled <- xgb.DMatrix(
  data = as.matrix(train_set_upsampled[, -which(names(train_set_upsampled) == "TARGET")]),
  label = train_set_upsampled$TARGET
)

str(train_set_upsampled$TARGET)
summary(train_set_upsampled$TARGET)
```


```{r}

# upsampled model with the best parameters from before 

xgb_model_upsampled <- xgboost(
  data = dtrain_upsampled,
  max_depth = 4,
  eta = 0.05,
  nrounds = 200,
  gamma = 1,               
  colsample_bytree = 0.6,   
  min_child_weight = 10,  
  subsample = 0.8,         
  objective = "binary:logistic", 
  eval_metric = "auc"
)

# predictions

xgb_predictions_upsampled <- predict(xgb_model_upsampled, newdata = dtest)
roc_curve_upsampled <- roc(test_target, xgb_predictions_upsampled)
auc_upsampled <- auc(roc_curve_upsampled)

print(paste("Upsampled XGBoost AUC:", auc_upsampled))
plot(roc_curve_upsampled, main = "ROC Curve Upsampled XGBoost", col = "red", lwd = 2)

```


Upsampled XGBoost AUC: 0.746583513744973, a bit of a drop, but we know we need to explore these scores further bc with the major class imbalance we want the model to be good at capturing the defaulters. 


```{r}

# threshold point, again this was altered to .6 but did not make major improvments
threshold <- 0.6

predicted_classes_upsampled <- ifelse(xgb_predictions_upsampled >= threshold, "Yes", "No")

test_target <- factor(test_target, levels = c("No", "Yes"))

# CM
confusion_matrix_upsampled <- table(Predicted = predicted_classes_upsampled, Actual = test_target)
print("Confusion Matrix (Upsampled):")
print(confusion_matrix_upsampled)

# ACC
accuracy_upsampled <- sum(diag(confusion_matrix_upsampled)) / sum(confusion_matrix_upsampled)
print(paste("Accuracy (Upsampled):", round(accuracy_upsampled, 4)))

precision_upsampled <- confusion_matrix_upsampled["Yes", "Yes"] / sum(confusion_matrix_upsampled["Yes", ])
recall_upsampled <- confusion_matrix_upsampled["Yes", "Yes"] / sum(confusion_matrix_upsampled[, "Yes"])
f1_score_upsampled <- 2 * (precision_upsampled * recall_upsampled) / (precision_upsampled + recall_upsampled)

print(paste("Precision (Upsampled):", round(precision_upsampled, 4)))
print(paste("Recall (Sensitivity, Upsampled):", round(recall_upsampled, 4)))
print(paste("F1-Score (Upsampled):", round(f1_score_upsampled, 4)))

# class distribution in the upsampled training dataset
prop.table(table(train_set_B$TARGET))  


```


ROC AUC comparision between upsampled and tuned.

```{r}
roc.test(roc_curve_xgb, roc_curve_xgb_tuned)
```


## Model Comparisons

```{r}

plot(roc_curve_xgb, main = "ROC Curve Comparison: Original vs Tuned", col = "red", lwd = 2)

lines(roc_curve_xgb_tuned, col = "blue", lwd = 2)

legend("bottomright", 
       legend = c("Original XGBoost", "Tuned XGBoost"), 
       col = c("red", "blue"), 
       lwd = 2)

```



## Visualizations for the Presentation:

AUC ROC curve comparison
```{r}

# Baseline model
plot(roc_curve_model1, main = "ROC Curve Comparison", col = "purple", lwd = 2)

# Add the glm with engineered features
plot(roc_curve_model3, col = "lightblue", lwd = 2, add = TRUE)

# random forest
plot(roc_curve_rf, col = "green", lwd = 2, add = TRUE)

#xgboost not tuned

plot(roc_curve_xgb, col = "orange", lwd = 2, add = TRUE)




#xgboost tuned
plot(roc_curve_xgb_tuned, col = "red", lwd = 2, add = TRUE)

#xgboost tuned with upsampling

plot(roc_curve_upsampled, col = "pink", lwd = 2, add = TRUE)



# Add a legend 
legend("bottomright", 
       legend = c("Baseline Model", "GLM with Engineered Features", "Random Forest Model", "XGBoost", "XGBoost with tuned parameters","XGBoost tuned with upsampled" ), 
       col = c("purple", "lightblue", "orange", "red","pink"), 
       lwd = 2)
```
0.7513 is the non upsampled model



Feature importance graph. 
```{r}
# importance matrix to a data frame
importance_df <- as.data.frame(importance_matrix)
top_features <- importance_df[1:25, ]


ggplot(top_features, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  geom_text(aes(label = round(Gain, 3)), hjust = -0.3, size = 3) + 
  labs(
    title = "Feature Importance from XGBoost Model",
    x = "Features",
    y = "Feature Importance Score (Gain)"
  ) +
  theme_minimal() +
  theme(
    plot.margin = margin(10, 20, 10, 20),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 8) 
  )

# Save the plot for presentation 
ggsave("feature_importance_plot.png", width = 12, height = 8, dpi = 300)
```



```{r}
png("roc_curve_comparison_adjusted.png", width = 1600, height = 1200, res = 300)

# baseline first ROC curve
plot(roc_curve_model1, main = "ROC Curve Comparison", col = "purple", lwd = 2)

# add other ROC curves
plot(roc_curve_model3, col = "lightblue", lwd = 2, add = TRUE)
plot(roc_curve_rf, col = "green", lwd = 2, add = TRUE)
plot(roc_curve_xgb, col = "orange", lwd = 2, add = TRUE)
plot(roc_curve_xgb_tuned, col = "red", lwd = 2, add = TRUE)
plot(roc_curve_upsampled, col = "pink", lwd = 2, add = TRUE)

legend(
  "bottomright", # Bottom-right placement
  legend = c(
    "Baseline Model", 
    "GLM(Engineered Features)", 
    "Random Forest Model", 
    "XGBoost", 
    "XGBoost with Tuned Parameters", 
    "Tuned and Upsampled XGBoost"
  ), 
  col = c("purple", "lightblue", "green", "orange", "red", "pink"), 
  lwd = 2, 
  cex = 0.6, # Make text smaller
  inset = c(0.05, 0.05), 
  bty = "n" 
)

dev.off()

```






## Kaggle Score

```{r}
# test data
Kaggle_test <- read.csv("/Users/u0847758/Desktop/School/home-credit-default-risk/application_test.csv")

# save ID in seperate column
Kaggle_test_ids <- Kaggle_test$SK_ID_CURR
```

```{r}
# extreme value replacement
Kaggle_test$DAYS_EMPLOYED[Kaggle_test$DAYS_EMPLOYED == 365243] <- NA
Kaggle_test$DAYS_EMPLOYED <- abs(Kaggle_test$DAYS_EMPLOYED)
```

```{r}
# add features we made
Kaggle_test <- Kaggle_test %>%
  select(all_of(c("EXT_SOURCE_1")), where(~ mean(is.na(.)) <= 0.48)) %>%
  mutate(
    Age = -DAYS_BIRTH / 365,
    EXT_SOURCE_1 = ifelse(is.na(EXT_SOURCE_1), median(App_train$EXT_SOURCE_1, na.rm = TRUE), EXT_SOURCE_1),
    EXT_SOURCE_2 = ifelse(is.na(EXT_SOURCE_2), median(App_train$EXT_SOURCE_2, na.rm = TRUE), EXT_SOURCE_2),
    EXT_SOURCE_3 = ifelse(is.na(EXT_SOURCE_3), median(App_train$EXT_SOURCE_3, na.rm = TRUE), EXT_SOURCE_3),
    DTI = AMT_CREDIT / AMT_INCOME_TOTAL,
    Loan_to_Credit_Score1 = AMT_CREDIT / EXT_SOURCE_1,
    Loan_to_Credit_Score2 = AMT_CREDIT / EXT_SOURCE_2,
    Loan_to_Credit_Score3 = AMT_CREDIT / EXT_SOURCE_3,
    Annuity_to_Income_Ratio = AMT_ANNUITY / AMT_INCOME_TOTAL,
    Credit_Duration_Ratio = AMT_CREDIT / AMT_ANNUITY,
    Per_Capita_Income = AMT_INCOME_TOTAL / CNT_FAM_MEMBERS,
    Per_Capita_Loan = AMT_CREDIT / CNT_FAM_MEMBERS
  )
```


```{r}
# align  features with the training set
Kaggle_test <- Kaggle_test[, colnames(train_set_B[, -which(names(train_set_B) == "TARGET")])]
```

```{r}
# columns in train_set_B (excluding TARGET)
train_features <- colnames(train_set_B[, -which(names(train_set_B) == "TARGET")])

# columns in Kaggle_test
test_features <- colnames(Kaggle_test)

# misaligned features?
missing_in_test <- setdiff(train_features, test_features)
extra_in_test <- setdiff(test_features, train_features)

cat("Missing in Kaggle_test:\n", missing_in_test, "\n")
cat("Extra in Kaggle_test:\n", extra_in_test, "\n")

```


```{r}
Kaggle_test <- Kaggle_test %>%
  mutate(
    All_EXT_Missing = ifelse(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3), 1, 0),
    EXT_SOURCE_1_Missing = ifelse(is.na(EXT_SOURCE_1), 1, 0),
    EXT_SOURCE_2_Missing = ifelse(is.na(EXT_SOURCE_2), 1, 0),
    EXT_SOURCE_3_Missing = ifelse(is.na(EXT_SOURCE_3), 1, 0)
  )
```

```{r}
# remove extra columns from Kaggle_test
Kaggle_test <- Kaggle_test[, train_features]

Kaggle_test <- Kaggle_test[, colnames(train_set_B[, -which(names(train_set_B) == "TARGET")])]

```


```{r}
# make character columns to factors and then to numeric
Kaggle_test <- Kaggle_test %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))
# matrix
Kaggle_test_matrix <- xgb.DMatrix(data = as.matrix(Kaggle_test))
```


```{r}
# prediction probabilities
kaggle_predictions <- predict(xgb_model_tuned, newdata = Kaggle_test_matrix)
```


```{r}
#  alignment
submission <- data.frame(SK_ID_CURR = Kaggle_test_ids, TARGET = kaggle_predictions)

# submission file for Kaggle site. 
write.csv(submission, "submission.csv", row.names = FALSE)
```





